{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":214432,"sourceType":"datasetVersion","datasetId":92290},{"sourceId":7174434,"sourceType":"datasetVersion","datasetId":4145682},{"sourceId":7194192,"sourceType":"datasetVersion","datasetId":4160414}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Переда запуском добавь коко2014 [датасет](https://www.kaggle.com/datasets/nadaibrahim/coco2014) в ноутбук)**","metadata":{}},{"cell_type":"code","source":"!pip install wandb\n!pip install bitsandbytes\n!pip install ruclip==0.0.2\n!pip install transformers==4.27.4\n!pip install pycocotools\n!pip install git+https://github.com/openai/CLIP.git\n!pip install open_clip_torch","metadata":{"execution":{"iopub.status.busy":"2023-12-13T21:59:38.959422Z","iopub.execute_input":"2023-12-13T21:59:38.960217Z","iopub.status.idle":"2023-12-13T22:01:40.700488Z","shell.execute_reply.started":"2023-12-13T21:59:38.960185Z","shell.execute_reply":"2023-12-13T22:01:40.699319Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.0)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.32)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.34.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.1.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nCollecting bitsandbytes\n  Obtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/d9/8d/b62d4fb02587e293e5b91b68bbcaa2d88c6a0360b622e9521d4bd07a20cd/bitsandbytes-0.41.3.post2-py3-none-any.whl.metadata\n  Downloading bitsandbytes-0.41.3.post2-py3-none-any.whl.metadata (9.8 kB)\nDownloading bitsandbytes-0.41.3.post2-py3-none-any.whl (92.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.41.3.post2\nCollecting ruclip==0.0.2\n  Downloading ruclip-0.0.2-py3-none-any.whl (14 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from ruclip==0.0.2) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from ruclip==0.0.2) (0.15.1)\nCollecting huggingface-hub==0.2.1 (from ruclip==0.0.2)\n  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting youtokentome~=1.0.6 (from ruclip==0.0.2)\n  Downloading youtokentome-1.0.6.tar.gz (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting more-itertools==8.12.0 (from ruclip==0.0.2)\n  Downloading more_itertools-8.12.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.2.1->ruclip==0.0.2) (3.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.2.1->ruclip==0.0.2) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.2.1->ruclip==0.0.2) (4.66.1)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.2.1->ruclip==0.0.2) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.2.1->ruclip==0.0.2) (4.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.2.1->ruclip==0.0.2) (21.3)\nRequirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.10/site-packages (from youtokentome~=1.0.6->ruclip==0.0.2) (8.1.7)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->ruclip==0.0.2) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->ruclip==0.0.2) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->ruclip==0.0.2) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->ruclip==0.0.2) (1.24.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->ruclip==0.0.2) (10.1.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub==0.2.1->ruclip==0.0.2) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->ruclip==0.0.2) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.2.1->ruclip==0.0.2) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.2.1->ruclip==0.0.2) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.2.1->ruclip==0.0.2) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.2.1->ruclip==0.0.2) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->ruclip==0.0.2) (1.3.0)\nBuilding wheels for collected packages: youtokentome\n  Building wheel for youtokentome (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for youtokentome: filename=youtokentome-1.0.6-cp310-cp310-linux_x86_64.whl size=193020 sha256=ad0e2e8c9fcf98ea64013032c5d386261a573c6f43dc7c9c0b2334f1c0c8e0b5\n  Stored in directory: /root/.cache/pip/wheels/df/85/f8/301d2ba45f43f30bed2fe413efa760bc726b8b660ed9c2900c\nSuccessfully built youtokentome\nInstalling collected packages: youtokentome, more-itertools, huggingface-hub, ruclip\n  Attempting uninstall: more-itertools\n    Found existing installation: more-itertools 10.1.0\n    Uninstalling more-itertools-10.1.0:\n      Successfully uninstalled more-itertools-10.1.0\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.17.3\n    Uninstalling huggingface-hub-0.17.3:\n      Successfully uninstalled huggingface-hub-0.17.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\ntransformers 4.35.0 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.2.1 more-itertools-8.12.0 ruclip-0.0.2 youtokentome-1.0.6\nCollecting transformers==4.27.4\n  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (3.12.2)\nCollecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.27.4)\n  Obtaining dependency information for huggingface-hub<1.0,>=0.11.0 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (2.31.0)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.4)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.4) (2023.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.4) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.27.4) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.4) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.4) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.4) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.4) (2023.7.22)\nDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, huggingface-hub, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.14.1\n    Uninstalling tokenizers-0.14.1:\n      Successfully uninstalled tokenizers-0.14.1\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.2.1\n    Uninstalling huggingface-hub-0.2.1:\n      Successfully uninstalled huggingface-hub-0.2.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.35.0\n    Uninstalling transformers-4.35.0:\n      Successfully uninstalled transformers-4.35.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nruclip 0.0.2 requires huggingface-hub==0.2.1, but you have huggingface-hub 0.19.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.19.4 tokenizers-0.13.3 transformers-4.27.4\nCollecting pycocotools\n  Obtaining dependency information for pycocotools from https://files.pythonhosted.org/packages/ba/64/0451cf41a00fd5ac4501de4ea0e395b7d909e09d665e56890b5d3809ae26/pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.24.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (10.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.7\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-nbh5ix38\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-nbh5ix38\n  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy (from clip==1.0)\n  Obtaining dependency information for ftfy from https://files.pythonhosted.org/packages/91/f8/dfa32d06cfcbdb76bc46e0f5d69c537de33f4cedb1a15cd4746ab45a6a26/ftfy-6.1.3-py3-none-any.whl.metadata\n  Downloading ftfy-6.1.3-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.8.8)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.15.1)\nCollecting wcwidth<0.3.0,>=0.2.12 (from ftfy->clip==1.0)\n  Obtaining dependency information for wcwidth<0.3.0,>=0.2.12 from https://files.pythonhosted.org/packages/31/b1/a59de0ad3aabb17523a39804f4c6df3ae87ead053a4e25362ae03d73d03a/wcwidth-0.2.12-py2.py3-none-any.whl.metadata\n  Downloading wcwidth-0.2.12-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nDownloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\nBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=947ef177e2481a85baecbdd22bf35b1e8222314ee81280b9ddbbe567c778053d\n  Stored in directory: /tmp/pip-ephem-wheel-cache-0j3x8vpv/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: wcwidth, ftfy, clip\n  Attempting uninstall: wcwidth\n    Found existing installation: wcwidth 0.2.6\n    Uninstalling wcwidth-0.2.6:\n      Successfully uninstalled wcwidth-0.2.6\nSuccessfully installed clip-1.0 ftfy-6.1.3 wcwidth-0.2.12\nCollecting open_clip_torch\n  Obtaining dependency information for open_clip_torch from https://files.pythonhosted.org/packages/7c/7f/952fdffa17b15d0c7c51a730860fcf4f4982528ecc753b190dcd46cc944b/open_clip_torch-2.23.0-py3-none-any.whl.metadata\n  Downloading open_clip_torch-2.23.0-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.15.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2023.8.8)\nRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (6.1.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (4.66.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.19.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.1.99)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (3.20.3)\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.9.10)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.2)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->open_clip_torch) (0.2.12)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2023.10.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (6.0.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (21.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm->open_clip_torch) (0.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (1.24.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (10.1.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->open_clip_torch) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\nDownloading open_clip_torch-2.23.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: open_clip_torch\nSuccessfully installed open_clip_torch-2.23.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Качаем РУССКИЙ капчеринг","metadata":{}},{"cell_type":"code","source":"#!rm -rf ru_capt.json\n##!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1uIO34T8d0ML23I30mcRFAD7niggm1kIt' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1uIO34T8d0ML23I30mcRFAD7niggm1kIt\" -O ru_capt.json && rm -rf /tmp/cookies.txt","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:01:40.702868Z","iopub.execute_input":"2023-12-13T22:01:40.703255Z","iopub.status.idle":"2023-12-13T22:01:40.708461Z","shell.execute_reply.started":"2023-12-13T22:01:40.703213Z","shell.execute_reply":"2023-12-13T22:01:40.707529Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### Качаем ембединги датасета","metadata":{}},{"cell_type":"code","source":"#!rm -rf train2014_emb.zip\n# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=11OpNsXhmtafzItkGxaTqMXqgL2dQycl6' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=11OpNsXhmtafzItkGxaTqMXqgL2dQycl6\" -O train2014_emb.zip && rm -rf /tmp/cookies.txt\n# !unzip train2014_emb.zip","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:01:40.709960Z","iopub.execute_input":"2023-12-13T22:01:40.710229Z","iopub.status.idle":"2023-12-13T22:01:40.719238Z","shell.execute_reply.started":"2023-12-13T22:01:40.710207Z","shell.execute_reply":"2023-12-13T22:01:40.718316Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#!rm -rf val2014_emb.zip\n# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1a-XuH0q5Ktlo6fIGFKQVkGEUkjiQ10X0' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1a-XuH0q5Ktlo6fIGFKQVkGEUkjiQ10X0\" -O val2014_emb.zip && rm -rf /tmp/cookies.txt\n# !unzip val2014_emb.zip\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:01:40.721463Z","iopub.execute_input":"2023-12-13T22:01:40.721774Z","iopub.status.idle":"2023-12-13T22:01:40.732072Z","shell.execute_reply.started":"2023-12-13T22:01:40.721739Z","shell.execute_reply":"2023-12-13T22:01:40.731163Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#import torchvision.datasets as dset\n#coco_train = dset.CocoDetection(\n#    root = \"/kaggle/input/coco2014/train2014/train2014\",\n#    annFile = \"/kaggle/input/coco2014/captions/annotations/captions_train2014.json\"\n#)\n#coco_val = dset.CocoDetection(\n#    root = \"/kaggle/input/coco2014/val2014/val2014\",\n#    annFile = \"/kaggle/input/coco2014/captions/annotations/captions_val2014.json\"\n#)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:01:40.733200Z","iopub.execute_input":"2023-12-13T22:01:40.733463Z","iopub.status.idle":"2023-12-13T22:01:40.741332Z","shell.execute_reply.started":"2023-12-13T22:01:40.733440Z","shell.execute_reply":"2023-12-13T22:01:40.740452Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as nnf\nfrom torch.utils.data import Dataset, DataLoader\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:01:40.742395Z","iopub.execute_input":"2023-12-13T22:01:40.742891Z","iopub.status.idle":"2023-12-13T22:01:42.367164Z","shell.execute_reply.started":"2023-12-13T22:01:40.742866Z","shell.execute_reply":"2023-12-13T22:01:42.366199Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm, trange\nimport os\nimport pickle\nimport sys\nimport argparse\nimport json\nfrom typing import Tuple, Optional, Union\nfrom torch.cuda.amp import autocast\n\nimport ruclip\nimport clip, open_clip\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:01:42.368740Z","iopub.execute_input":"2023-12-13T22:01:42.369212Z","iopub.status.idle":"2023-12-13T22:01:45.724404Z","shell.execute_reply.started":"2023-12-13T22:01:42.369177Z","shell.execute_reply":"2023-12-13T22:01:45.723578Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Config, GPT2Model\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:01:45.725484Z","iopub.execute_input":"2023-12-13T22:01:45.725849Z","iopub.status.idle":"2023-12-13T22:01:45.933781Z","shell.execute_reply.started":"2023-12-13T22:01:45.725823Z","shell.execute_reply":"2023-12-13T22:01:45.932833Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:01:45.935042Z","iopub.execute_input":"2023-12-13T22:01:45.935527Z","iopub.status.idle":"2023-12-13T22:01:55.824308Z","shell.execute_reply.started":"2023-12-13T22:01:45.935499Z","shell.execute_reply":"2023-12-13T22:01:55.823482Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"#from multilingual_clip import pt_multilingual_clip","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:01:55.828409Z","iopub.execute_input":"2023-12-13T22:01:55.828981Z","iopub.status.idle":"2023-12-13T22:01:55.832872Z","shell.execute_reply.started":"2023-12-13T22:01:55.828952Z","shell.execute_reply":"2023-12-13T22:01:55.832048Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"manualSeed = 1337\n#manualSeed = random.randint(1, 10000) # use if you want new results\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:01:55.834112Z","iopub.execute_input":"2023-12-13T22:01:55.834364Z","iopub.status.idle":"2023-12-13T22:01:55.884153Z","shell.execute_reply.started":"2023-12-13T22:01:55.834341Z","shell.execute_reply":"2023-12-13T22:01:55.883366Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n        super(MLP, self).__init__()\n        layers = []\n        for i in range(len(sizes) - 1):\n            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n            if i < len(sizes) - 2:\n                layers.append(act())\n        self.model = nn.Sequential(*layers)\n    \n    @autocast()  \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:42.862711Z","iopub.execute_input":"2023-12-13T22:02:42.863463Z","iopub.status.idle":"2023-12-13T22:02:42.871393Z","shell.execute_reply.started":"2023-12-13T22:02:42.863424Z","shell.execute_reply":"2023-12-13T22:02:42.870234Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class MlpTransformer(nn.Module):\n    def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):\n        super().__init__()\n        out_d = out_d if out_d is not None else in_dim\n        self.fc1 = nn.Linear(in_dim, h_dim)\n        self.act = act\n        self.fc2 = nn.Linear(h_dim, out_d)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:43.058296Z","iopub.execute_input":"2023-12-13T22:02:43.059135Z","iopub.status.idle":"2023-12-13T22:02:43.065717Z","shell.execute_reply.started":"2023-12-13T22:02:43.059100Z","shell.execute_reply":"2023-12-13T22:02:43.064867Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n\n    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim_self // num_heads\n        self.scale = head_dim ** -0.5\n        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n        self.project = nn.Linear(dim_self, dim_self)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, y=None, mask=None):\n        y = y if y is not None else x\n        b, n, c = x.shape\n        _, m, d = y.shape\n        # b n h dh\n        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n        # b m 2 h dh\n        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n        if mask is not None:\n            if mask.dim() == 2:\n                mask = mask.unsqueeze(1)\n            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n        attention = attention.softmax(dim=2)\n        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n        out = self.project(out)\n        return out, attention","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:43.226775Z","iopub.execute_input":"2023-12-13T22:02:43.227397Z","iopub.status.idle":"2023-12-13T22:02:43.237932Z","shell.execute_reply.started":"2023-12-13T22:02:43.227366Z","shell.execute_reply":"2023-12-13T22:02:43.236961Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class TransformerLayer(nn.Module):\n\n    def forward_with_attention(self, x, y=None, mask=None):\n        x_, attention = self.attn(self.norm1(x), y, mask)\n        x = x + x_\n        x = x + self.mlp(self.norm2(x))\n        return x, attention\n\n    def forward(self, x, y=None, mask=None):\n        x = x + self.attn(self.norm1(x), y, mask)[0]\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n                 norm_layer: nn.Module = nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim_self)\n        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n        self.norm2 = norm_layer(dim_self)\n        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n\n\nclass Transformer(nn.Module):\n\n    def forward_with_attention(self, x, y=None, mask=None):\n        attentions = []\n        for layer in self.layers:\n            x, att = layer.forward_with_attention(x, y, mask)\n            attentions.append(att)\n        return x, attentions\n\n    def forward(self, x, y=None, mask=None):\n        for i, layer in enumerate(self.layers):\n            if i % 2 == 0 and self.enc_dec: # cross\n                x = layer(x, y)\n            elif self.enc_dec:  # self\n                x = layer(x, x, mask)\n            else:  # self or cross\n                x = layer(x, y, mask)\n        return x\n\n    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,\n                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):\n        super(Transformer, self).__init__()\n        dim_ref = dim_ref if dim_ref is not None else dim_self\n        self.enc_dec = enc_dec\n        if enc_dec:\n            num_layers = num_layers * 2\n        layers = []\n        for i in range(num_layers):\n            if i % 2 == 0 and enc_dec:  # cross\n                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n            elif enc_dec:  # self\n                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n            else:  # self or cross\n                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n        self.layers = nn.ModuleList(layers)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:43.613259Z","iopub.execute_input":"2023-12-13T22:02:43.613885Z","iopub.status.idle":"2023-12-13T22:02:43.629270Z","shell.execute_reply.started":"2023-12-13T22:02:43.613857Z","shell.execute_reply":"2023-12-13T22:02:43.628265Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class TransformerMapper(nn.Module):\n\n    def forward(self, x):\n        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n        prefix = torch.cat((x, prefix), dim=1)\n        out = self.transformer(prefix)[:, self.clip_length:]\n        return out\n\n    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n        super(TransformerMapper, self).__init__()\n        self.clip_length = clip_length\n        self.transformer = Transformer(dim_embedding, 8, num_layers)\n        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:43.846123Z","iopub.execute_input":"2023-12-13T22:02:43.846421Z","iopub.status.idle":"2023-12-13T22:02:43.853949Z","shell.execute_reply.started":"2023-12-13T22:02:43.846395Z","shell.execute_reply":"2023-12-13T22:02:43.853052Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def freeze(\n    model,\n    freeze_emb=False,\n    freeze_ln=False,\n    freeze_attn=True,\n    freeze_ff=True,\n    freeze_other=False,\n):\n    \n    for name, p in model.named_parameters():\n    # freeze all parameters except the layernorm and positional embeddings\n        name = name.lower()\n        if 'ln' in name or 'norm' in name:\n            p.requires_grad = not freeze_ln\n        elif 'embeddings' in name:\n            p.requires_grad = not freeze_emb\n        elif 'mlp' in name:\n            p.requires_grad = not freeze_ff\n        elif 'attn' in name:\n            p.requires_grad = not freeze_attn\n        else:\n            p.requires_grad = not freeze_other\n           \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:44.120147Z","iopub.execute_input":"2023-12-13T22:02:44.120793Z","iopub.status.idle":"2023-12-13T22:02:44.127403Z","shell.execute_reply.started":"2023-12-13T22:02:44.120760Z","shell.execute_reply":"2023-12-13T22:02:44.126270Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from enum import Enum\nclass MappingType(Enum):\n    MLP = 'mlp'\n    Transformer = 'transformer'","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:44.501160Z","iopub.execute_input":"2023-12-13T22:02:44.501440Z","iopub.status.idle":"2023-12-13T22:02:44.506031Z","shell.execute_reply.started":"2023-12-13T22:02:44.501416Z","shell.execute_reply":"2023-12-13T22:02:44.505075Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"gpt_model_name = 'sberbank-ai/rugpt3medium_based_on_gpt2'\nclass ClipCaptionModel(nn.Module):\n    def __init__(\n        self,\n        prefix_length: int,\n        clip_length: Optional[int] = None,\n        prefix_size: int = 512,\n        num_layers: int = 8,\n        mapping_type: MappingType = MappingType.MLP\n    ):\n        super(ClipCaptionModel, self).__init__()\n        self.prefix_length = prefix_length\n\n        self.gpt = GPT2LMHeadModel.from_pretrained(gpt_model_name)\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n\n        if mapping_type == MappingType.MLP:\n            self.clip_project = MLP((\n                prefix_size,\n                self.gpt_embedding_size * prefix_length // 2,\n                self.gpt_embedding_size * prefix_length\n            ))\n        else:\n            self.clip_project = TransformerMapper(\n                prefix_size,\n                self.gpt_embedding_size,\n                prefix_length,\n                clip_length, \n                num_layers\n            )\n\n        \n    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n    \n    @autocast() \n    def forward(\n        self,        \n        tokens: torch.Tensor,\n        prefix: torch.Tensor,\n        mask: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None\n    ):\n        embedding_text = self.gpt.transformer.wte(tokens)\n        prefix_projections = self.clip_project(\n            prefix.float()\n        ).view(-1, self.prefix_length, self.gpt_embedding_size)\n\n        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n\n        if labels is not None:\n            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n            labels = torch.cat((dummy_token, tokens), dim=1)\n        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask, output_hidden_states = True)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:45.040972Z","iopub.execute_input":"2023-12-13T22:02:45.041593Z","iopub.status.idle":"2023-12-13T22:02:45.052819Z","shell.execute_reply.started":"2023-12-13T22:02:45.041566Z","shell.execute_reply":"2023-12-13T22:02:45.051889Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"class ClipCaptionPrefix(ClipCaptionModel):\n    def parameters(self, recurse: bool = True):\n        return self.clip_project.parameters()\n\n    def train(self, mode: bool = True):\n        super(ClipCaptionPrefix, self).train(mode)\n        self.gpt.eval()\n        return self","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:45.511089Z","iopub.execute_input":"2023-12-13T22:02:45.511350Z","iopub.status.idle":"2023-12-13T22:02:45.516908Z","shell.execute_reply.started":"2023-12-13T22:02:45.511328Z","shell.execute_reply":"2023-12-13T22:02:45.515905Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"path_emb_train_coco = f\"/kaggle/input/coco-ru-image-and-capt/Features_train_coco_ru_vitb16_82783.pkl\"\npath_emb_val_coco = f\"/kaggle/input/coco-ru-image-and-capt/Features_val_coco_ru_vitb16.pkl\"","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:46.045347Z","iopub.execute_input":"2023-12-13T22:02:46.045667Z","iopub.status.idle":"2023-12-13T22:02:46.049727Z","shell.execute_reply.started":"2023-12-13T22:02:46.045638Z","shell.execute_reply":"2023-12-13T22:02:46.048782Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import io\nclass CPU_Unpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        if module == 'torch.storage' and name == '_load_from_bytes':\n            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n        else: return super().find_class(module, name)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:46.650190Z","iopub.execute_input":"2023-12-13T22:02:46.650486Z","iopub.status.idle":"2023-12-13T22:02:46.655804Z","shell.execute_reply.started":"2023-12-13T22:02:46.650461Z","shell.execute_reply":"2023-12-13T22:02:46.654961Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"class ClipCocoDataset(Dataset):\n    def __init__(\n        self,\n        data_path: str,\n        prefix_length=30,\n        model_type = gpt_model_name,\n        normalize_prefix=False,\n        train=True,\n    ):\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n        self.prefix_length = prefix_length\n        self.normalize_prefix = normalize_prefix\n        if train:\n            with open(data_path, 'rb') as f:\n                all_data = CPU_Unpickler(f).load() #pickle.load(f)\n            print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n        else:\n            with open(data_path, 'rb') as f:\n                all_data = CPU_Unpickler(f).load() #pickle.load(f)\n            print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n\n        sys.stdout.flush()\n        self.prefixes = all_data[\"clip_embedding\"]\n        captions_raw = all_data[\"captions\"]\n        \n        self.captions = captions_raw\n\n        self.image_id = all_data[\"path_images\"]\n\n        self.captions_tokens = []\n        self.caption2embedding = []\n        max_seq_len = 0\n        i = 0\n        for caption in tqdm(captions_raw):\n            self.captions_tokens.append(\n                torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64)\n            )\n            self.caption2embedding.append(self.prefixes[i])\n            i += 1\n            max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n\n        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n\n    def get_image(self, item):\n        if self.train:\n            path_img = f\"/kaggle/input/train2014/train2014/{self.image_id[item]}\"\n        else:\n            path_img = f\"/kaggle/input/val2014/val2014/{self.image_id[item]}\"\n            \n        image = cv2.imread(path_img)\n        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        image.thumbnail((196, 196), Image.Resampling.LANCZOS)\n        return image\n    \n    def pad_tokens(self, item: int):\n        tokens = self.captions_tokens[item]\n        padding = self.max_seq_len - tokens.shape[0]\n        if padding > 0:\n            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            #self.captions_tokens[item] = tokens\n        elif padding < 0:\n            tokens = tokens[:self.max_seq_len]\n            #self.captions_tokens[item] = tokens\n        mask = tokens.ge(0)  # mask is zero where we out of sequence\n        tokens[~mask] = 0\n        mask = mask.float()\n        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n        return tokens, mask\n    \n    def __len__(self) -> int:\n        return len(self.captions_tokens)\n\n    def __getitem__(self, item):\n        tokens, mask = self.pad_tokens(item)\n        prefix = self.prefixes[item]\n        if self.normalize_prefix:\n            prefix = prefix.float()\n            prefix = prefix / prefix.norm(2, -1)\n        return tokens, mask, prefix","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:17:55.899263Z","iopub.execute_input":"2023-12-13T22:17:55.899906Z","iopub.status.idle":"2023-12-13T22:17:55.918973Z","shell.execute_reply.started":"2023-12-13T22:17:55.899870Z","shell.execute_reply":"2023-12-13T22:17:55.917818Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"def save_config(args: argparse.Namespace):\n    config = {}\n    for key, item in args._get_kwargs():\n        config[key] = item\n    out_path = os.path.join(args.out_dir, f\"{args.prefix}.json\")\n    with open(out_path, 'w') as outfile:\n        json.dump(config, outfile)\n\n\ndef load_model(config_path: str, epoch_or_latest: Union[str, int] = '_latest'):\n    with open(config_path) as f:\n        config = json.load(f)\n    parser = argparse.ArgumentParser()\n    parser.set_defaults(**config)\n    args = parser.parse_args()\n    if type(epoch_or_latest) is int:\n        epoch_or_latest = f\"-{epoch_or_latest:03d}\"\n    model_path = os.path.join(args.out_dir, f\"{args.prefix}{epoch_or_latest}.pt\")\n    if args.only_prefix:\n        model = ClipCaptionPrefix(args.prefix_length)\n    else:\n        model = ClipCaptionModel(args.prefix_length)\n    if os.path.isfile(model_path):\n        print(f\"loading model from {model_path}\")\n        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n    else:\n        print(f\"{model_path} is not exist\")\n    return model, parser\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:49.936543Z","iopub.execute_input":"2023-12-13T22:02:49.936880Z","iopub.status.idle":"2023-12-13T22:02:49.945801Z","shell.execute_reply.started":"2023-12-13T22:02:49.936856Z","shell.execute_reply":"2023-12-13T22:02:49.944920Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"torch.cuda.get_device_name(0)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:52.565644Z","iopub.execute_input":"2023-12-13T22:02:52.566512Z","iopub.status.idle":"2023-12-13T22:02:52.575520Z","shell.execute_reply.started":"2023-12-13T22:02:52.566469Z","shell.execute_reply":"2023-12-13T22:02:52.574628Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"'Tesla P100-PCIE-16GB'"},"metadata":{}}]},{"cell_type":"code","source":"torch.cuda.mem_get_info()[1] / 1024**3","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:52.763362Z","iopub.execute_input":"2023-12-13T22:02:52.763668Z","iopub.status.idle":"2023-12-13T22:02:57.569102Z","shell.execute_reply.started":"2023-12-13T22:02:52.763643Z","shell.execute_reply":"2023-12-13T22:02:57.568111Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"15.8992919921875"},"metadata":{}}]},{"cell_type":"code","source":"import bitsandbytes as bnb","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:02:57.570644Z","iopub.execute_input":"2023-12-13T22:02:57.570926Z","iopub.status.idle":"2023-12-13T22:02:57.807916Z","shell.execute_reply.started":"2023-12-13T22:02:57.570900Z","shell.execute_reply":"2023-12-13T22:02:57.806913Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## **TRAIN**","metadata":{}},{"cell_type":"markdown","source":"**Запусти ячейку и появится окошко для ввода токена)**","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:04:20.693884Z","iopub.execute_input":"2023-12-13T22:04:20.694773Z","iopub.status.idle":"2023-12-13T22:04:36.600363Z","shell.execute_reply.started":"2023-12-13T22:04:20.694738Z","shell.execute_reply":"2023-12-13T22:04:36.599472Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import wandb\n\nwandb.init(project=\"ClipCap_NAS\", name=\"SS-RUCLIP&GPT-2-BLEU\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:07:13.699447Z","iopub.execute_input":"2023-12-13T22:07:13.700264Z","iopub.status.idle":"2023-12-13T22:07:44.727810Z","shell.execute_reply.started":"2023-12-13T22:07:13.700226Z","shell.execute_reply":"2023-12-13T22:07:44.726937Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrbeketov\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231213_220713-ld7pwt45</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rbeketov/ClipCap_NAS/runs/ld7pwt45' target=\"_blank\">SS-RUCLIP&GPT-2-BLEU</a></strong> to <a href='https://wandb.ai/rbeketov/ClipCap_NAS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rbeketov/ClipCap_NAS' target=\"_blank\">https://wandb.ai/rbeketov/ClipCap_NAS</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rbeketov/ClipCap_NAS/runs/ld7pwt45' target=\"_blank\">https://wandb.ai/rbeketov/ClipCap_NAS/runs/ld7pwt45</a>"},"metadata":{}},{"execution_count":49,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rbeketov/ClipCap_NAS/runs/ld7pwt45?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7e67363109d0>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers.optimization import Adafactor, AdafactorSchedule\n\nimport os\nimport pickle\nimport sys\nimport argparse\n\nfrom typing import Tuple, Optional, Union\nfrom torch.cuda.amp import autocast\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:13:33.211343Z","iopub.execute_input":"2023-12-13T22:13:33.211994Z","iopub.status.idle":"2023-12-13T22:13:33.219961Z","shell.execute_reply.started":"2023-12-13T22:13:33.211960Z","shell.execute_reply":"2023-12-13T22:13:33.218647Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:13:33.375517Z","iopub.execute_input":"2023-12-13T22:13:33.376380Z","iopub.status.idle":"2023-12-13T22:13:33.381858Z","shell.execute_reply.started":"2023-12-13T22:13:33.376341Z","shell.execute_reply":"2023-12-13T22:13:33.380831Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, load_metric","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:13:33.531411Z","iopub.execute_input":"2023-12-13T22:13:33.531786Z","iopub.status.idle":"2023-12-13T22:13:33.537784Z","shell.execute_reply.started":"2023-12-13T22:13:33.531755Z","shell.execute_reply":"2023-12-13T22:13:33.536552Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\ndef calc_bleu(y_pred, y_true):\n    references = [[reference.split()] for reference in y_true]\n    hypotheses = [hypothesis.split() for hypothesis in y_pred]\n    # Рассчитываем BLEU-4\n    bleu_score = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n    return bleu_score*100","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:13:33.699803Z","iopub.execute_input":"2023-12-13T22:13:33.700174Z","iopub.status.idle":"2023-12-13T22:13:33.707676Z","shell.execute_reply.started":"2023-12-13T22:13:33.700144Z","shell.execute_reply":"2023-12-13T22:13:33.706663Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"def sup_train(\n    train_dataset: ClipCocoDataset,\n    train_dataloader,\n    #valid_dataset: ClipCocoDataset,\n    model: ClipCaptionModel,\n    optimizer,\n    scheduler,\n    args,\n    warmup_steps: int = 5000,\n    output_dir: str = \".\",\n    output_prefix: str = \"\"\n):\n    #\n    batch_size = args.bs\n    epochs = args.epochs\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    model = freeze(model)\n\n\n    model.train()\n\n    mean_epoch_train_loss = []\n    mean_bleu_train_epoch = []\n    \n    \n    for epoch in range(epochs):\n        loss_train_epoch = []\n        bleu_train_epoch = []\n        print(f\">>> Training epoch {epoch+1}\")\n        sys.stdout.flush()\n        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n        step=0\n        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n            model.zero_grad()\n            step += 1\n            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.bfloat16)\n            \n            outputs = model(tokens, prefix, mask)\n            logits = outputs.logits[:, train_dataset.prefix_length - 1: -1]\n\n            loss = nnf.cross_entropy(\n                logits.reshape(-1, logits.shape[-1]),\n                tokens.flatten().to(torch.int64),\n                ignore_index=0\n            )\n\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            optimizer.zero_grad()\n            progress.set_postfix({\"sup_loss_train\": loss.item()})\n            \n\n            loss_train_epoch.append(loss.item())\n\n            if step % 500 == 0:\n                wandb.log({\"sup_loss_train\":  loss.item()})\n            \n            if step % 1000 == 0:\n                with torch.no_grad():\n                    # BLEU-4\n                    logits_cpu = logits.cpu()\n                    tokens_cpu = tokens.cpu()\n                    generated_texts = []\n                    real_text = []\n                    for b in range(batch_size):\n                        generated_text_batch = train_dataset.tokenizer.decode(logits_cpu[b].argmax(dim=-1).tolist())\n                        first_dot_index = generated_text_batch.find('.')\n                        if first_dot_index != -1:\n                            generated_texts.append(generated_text_batch[35:first_dot_index + 1])\n                        else:\n                            generated_texts.append(generated_text_batch[35:])\n                        \n                        real_text_batch = train_dataset.tokenizer.decode(tokens_cpu[b].tolist())\n                        first_pad_index = real_text_batch.find('<pad>')\n                        if first_pad_index != -1:\n                            real_text.append(real_text_batch[35:first_pad_index])\n                        else:\n                            real_text.append(real_text_batch[35:])\n                    \n                    bleu = calc_bleu(generated_texts, real_text)\n                    wandb.log({\"bleu-4 train\":  bleu})\n                    bleu_train_epoch.append(bleu)\n                    \n\n            progress.update()\n            if (idx + 1) % 7000 == 0:\n                torch.save(\n                    model.state_dict(),\n                    os.path.join(output_dir, f\"{output_prefix}_latest_gpt2_medium.pt\"),\n                )\n        progress.close()\n        if epoch % args.save_every == 0:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}-{(epoch+1):03d}_gpt2_medium.pt\"),\n            )\n        mean_epoch_train_loss.append(np.mean(loss_train_epoch))\n        mean_bleu_train_epoch.append(np.mean(bleu_train_epoch))\n        \n        wandb.log({\"mean_epoch_sup_train_loss\": mean_epoch_train_loss[-1]})\n        wandb.log({\"mean_bleu_sup_train_epoch\": mean_bleu_train_epoch[-1]})\n\n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:13:34.652901Z","iopub.execute_input":"2023-12-13T22:13:34.653650Z","iopub.status.idle":"2023-12-13T22:13:34.674927Z","shell.execute_reply.started":"2023-12-13T22:13:34.653619Z","shell.execute_reply":"2023-12-13T22:13:34.674059Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"class Args():\n    def __init__(self):\n        self.backbone = gpt_model_name\n        self.train_data = \"/kaggle/input/coco2014-ru-clip-embeddings/embeddings_ru_clip_train.pkl\"\n        self.valid_data = \"/kaggle/input/coco2014-ru-clip-embeddings/embeddings_ru_clip_valid.pkl\"\n        self.out_dir = 'checkpoints'\n        self.prefix = 'clipru_ss_adamw'\n        self.epochs = 1\n        self.save_every = 1\n        self.prefix_length = 30\n        self.bs = 3\n        self.only_prefix = False\n        self.lr = 2e-5\n        self.warmup_steps = 5000\nargs = Args()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:14:23.577805Z","iopub.execute_input":"2023-12-13T22:14:23.578590Z","iopub.status.idle":"2023-12-13T22:14:23.585900Z","shell.execute_reply.started":"2023-12-13T22:14:23.578557Z","shell.execute_reply":"2023-12-13T22:14:23.584655Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"train_dataset = ClipCocoDataset(args.train_data, args.prefix_length, train=True)\n#valid_dataset = ClipCocoDataset(args.valid_data, args.prefix_length, train=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:18:03.758476Z","iopub.execute_input":"2023-12-13T22:18:03.758837Z","iopub.status.idle":"2023-12-13T22:20:31.044556Z","shell.execute_reply.started":"2023-12-13T22:18:03.758809Z","shell.execute_reply":"2023-12-13T22:20:31.043439Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"Data size is 414113\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 414113/414113 [02:17<00:00, 3010.57it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"#!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1BGVig1avA29Ul8M1kvQPn66wv0q6CZFz' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1BGVig1avA29Ul8M1kvQPn66wv0q6CZFz\" -O last_weights.zip && rm -rf /tmp/cookies.txt\n#!unzip last_weights.zip","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:11:42.752823Z","iopub.execute_input":"2023-12-13T22:11:42.753569Z","iopub.status.idle":"2023-12-13T22:12:36.889708Z","shell.execute_reply.started":"2023-12-13T22:11:42.753533Z","shell.execute_reply":"2023-12-13T22:12:36.888385Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"--2023-12-13 22:11:44--  https://docs.google.com/uc?export=download&confirm=t&id=1BGVig1avA29Ul8M1kvQPn66wv0q6CZFz\nResolving docs.google.com (docs.google.com)... 74.125.135.100, 74.125.135.139, 74.125.135.102, ...\nConnecting to docs.google.com (docs.google.com)|74.125.135.100|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://doc-10-3g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ngm6783svrrfeb021pr64c06ilhn3a94/1702505475000/17733576292032671640/*/1BGVig1avA29Ul8M1kvQPn66wv0q6CZFz?e=download&uuid=4a9c589a-e52d-41b6-a4f2-5c1c6155f949 [following]\nWarning: wildcards not supported in HTTP.\n--2023-12-13 22:11:44--  https://doc-10-3g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ngm6783svrrfeb021pr64c06ilhn3a94/1702505475000/17733576292032671640/*/1BGVig1avA29Ul8M1kvQPn66wv0q6CZFz?e=download&uuid=4a9c589a-e52d-41b6-a4f2-5c1c6155f949\nResolving doc-10-3g-docs.googleusercontent.com (doc-10-3g-docs.googleusercontent.com)... 74.125.197.132, 2607:f8b0:400e:c03::84\nConnecting to doc-10-3g-docs.googleusercontent.com (doc-10-3g-docs.googleusercontent.com)|74.125.197.132|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2685851844 (2.5G) [application/x-zip-compressed]\nSaving to: ‘last_weights.zip’\n\nlast_weights.zip    100%[===================>]   2.50G   106MB/s    in 26s     \n\n2023-12-13 22:12:11 (97.1 MB/s) - ‘last_weights.zip’ saved [2685851844/2685851844]\n\nArchive:  last_weights.zip\n  inflating: 20bs_adamw-001_gpt2_medium.pt  ^C\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.config = {\n  \"learning_rate\": args.lr,\n  \"epochs\": args.epochs,\n  \"batch_size\": args.bs\n}\n\n\n\nmodel = ClipCaptionModel(args.prefix_length)\nmodel_path = '20bs_adamw-001_gpt2_medium.pt'\nmodel.load_state_dict(torch.load(model_path, map_location='cpu')) \nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T22:21:28.788062Z","iopub.execute_input":"2023-12-13T22:21:28.788431Z","iopub.status.idle":"2023-12-13T22:21:45.442229Z","shell.execute_reply.started":"2023-12-13T22:21:28.788400Z","shell.execute_reply":"2023-12-13T22:21:45.440881Z"},"trusted":true},"execution_count":81,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/761 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe874092d2ef4f6293e6c5478a4f06dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.73G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c90bd01376c4e0188f199e66959d313"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[81], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m ClipCaptionModel(args\u001b[38;5;241m.\u001b[39mprefix_length)\n\u001b[1;32m     10\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20bs_adamw-001_gpt2_medium.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 11\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m) \n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:797\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m--> 797\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    798\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m    799\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    800\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    801\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:283\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"],"ename":"RuntimeError","evalue":"PytorchStreamReader failed reading zip archive: failed finding central directory","output_type":"error"}]},{"cell_type":"code","source":"len(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:24:20.827826Z","iopub.execute_input":"2023-12-12T17:24:20.828370Z","iopub.status.idle":"2023-12-12T17:24:20.838517Z","shell.execute_reply.started":"2023-12-12T17:24:20.828273Z","shell.execute_reply":"2023-12-12T17:24:20.837364Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"414113"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\n\ndef sup_unsup_train_dataset(dataset, val_split):\n    sup_idx, unsup_idx = train_test_split(list(range(len(dataset))), test_size=1-val_split, shuffle=False)\n    return Subset(dataset, sup_idx), Subset(dataset, unsup_idx)\n\nsup_train_dataset, unsup_train_dataset = sup_unsup_train_dataset(train_dataset, 0.3)\n\nsup_train_dataset.prefix_length = train_dataset.prefix_length\nunsup_train_dataset.prefix_length = train_dataset.prefix_length\nsup_train_dataset.tokenizer = train_dataset.tokenizer\nunsup_train_dataset.tokenizer = train_dataset.tokenizer\n\nprint(len(sup_train_dataset))\nprint(len(unsup_train_dataset))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:24:20.839912Z","iopub.execute_input":"2023-12-12T17:24:20.840267Z","iopub.status.idle":"2023-12-12T17:24:20.946169Z","shell.execute_reply.started":"2023-12-12T17:24:20.840205Z","shell.execute_reply":"2023-12-12T17:24:20.945053Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"124233\n289880\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\nsup_train_dataloader = DataLoader(\n    sup_train_dataset,\n    batch_size=args.bs,\n    shuffle=True,\n    drop_last=True,\n)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:24:20.947432Z","iopub.execute_input":"2023-12-12T17:24:20.947743Z","iopub.status.idle":"2023-12-12T17:24:20.954288Z","shell.execute_reply.started":"2023-12-12T17:24:20.947715Z","shell.execute_reply":"2023-12-12T17:24:20.953045Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nsup_optimizer = AdamW(\n         model.parameters(),\n         lr=args.lr,\n     )\n\n sup_scheduler = get_linear_schedule_with_warmup(\n     sup_optimizer,\n     num_warmup_steps=args.warmup_steps,\n     num_training_steps=args.epochs * len(sup_train_dataloader)\n )\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-12-12T09:00:22.936630Z","iopub.execute_input":"2023-12-12T09:00:22.937021Z","iopub.status.idle":"2023-12-12T09:00:22.950109Z","shell.execute_reply.started":"2023-12-12T09:00:22.936987Z","shell.execute_reply":"2023-12-12T09:00:22.948923Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"#import torch\n#import gc\n#gc.collect()\n#torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:10:34.509387Z","iopub.execute_input":"2023-12-12T17:10:34.510132Z","iopub.status.idle":"2023-12-12T17:10:35.342862Z","shell.execute_reply.started":"2023-12-12T17:10:34.510088Z","shell.execute_reply":"2023-12-12T17:10:35.341553Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"20bs_adamw:   4%|▍         | 4205/96626 [22:10<8:07:28,  3.16it/s, unsup_loss_train=-87.6]\n","output_type":"stream"}]},{"cell_type":"code","source":"sys.stdout.flush()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T13:15:12.176478Z","iopub.execute_input":"2023-12-12T13:15:12.176847Z","iopub.status.idle":"2023-12-12T13:15:12.182902Z","shell.execute_reply.started":"2023-12-12T13:15:12.176812Z","shell.execute_reply":"2023-12-12T13:15:12.181810Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# print(\"Train both prefix and GPT2\")\n# sys.stdout.flush()\n# model = sup_train(\n#     sup_train_dataset,\n#     sup_train_dataloader,\n#     model,\n#     sup_optimizer,\n#     sup_scheduler,\n#     args,\n#     warmup_steps=args.warmup_steps,\n#     output_dir=args.out_dir,\n#     output_prefix=args.prefix\n# )","metadata":{"execution":{"iopub.status.busy":"2023-12-12T09:00:49.022833Z","iopub.execute_input":"2023-12-12T09:00:49.023221Z","iopub.status.idle":"2023-12-12T11:38:05.209891Z","shell.execute_reply.started":"2023-12-12T09:00:49.023189Z","shell.execute_reply":"2023-12-12T11:38:05.208503Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Train both prefix and GPT2\n>>> Training epoch 1\n","output_type":"stream"},{"name":"stderr","text":"20bs_adamw:   2%|▏         | 999/41411 [03:48<2:32:08,  4.43it/s, sup_loss_train=2.15]/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n20bs_adamw:   5%|▍         | 1999/41411 [07:34<2:28:36,  4.42it/s, sup_loss_train=1.72] /opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n20bs_adamw:  10%|▉         | 3999/41411 [15:06<2:20:44,  4.43it/s, sup_loss_train=2.24] /opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n20bs_adamw: 100%|██████████| 41411/41411 [2:37:10<00:00,  4.39it/s, sup_loss_train=1.66]   \n","output_type":"stream"}]},{"cell_type":"code","source":"# with torch.no_grad():\n#     for idx, (tokens, mask, prefix) in enumerate(sup_train_dataloader):\n#         model.zero_grad()\n#         tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.bfloat16)\n\n#         outputs = model(tokens, prefix, mask)\n#         logits = outputs.logits[:, train_dataset.prefix_length - 1: -1]\n\n#         # BLEU-4\n#         logits_cpu = logits.cpu()\n#         tokens_cpu = tokens.cpu()\n#         generated_texts = []\n#         real_text = []\n#         for b in range(args.bs):\n#             generated_text_batch = train_dataset.tokenizer.decode(logits_cpu[b].argmax(dim=-1).tolist())\n#             first_dot_index = generated_text_batch.find('.')\n#             if first_dot_index != -1:\n#                 generated_texts.append(generated_text_batch[35:first_dot_index + 1])\n#             else:\n#                 generated_texts.append(generated_text_batch[35:])\n\n#             real_text_batch = train_dataset.tokenizer.decode(tokens_cpu[b].tolist())\n#             first_pad_index = real_text_batch.find('<pad>')\n#             if first_pad_index != -1:\n#                 real_text.append(real_text_batch[35:first_pad_index])\n#             else:\n#                 real_text.append(real_text_batch[35:])\n                \n#             print('Real: ', real_text_batch)\n#             print('Generated: ', generated_texts)\n#             print('Bleu:', calc_bleu(generated_texts, real_text))\n#             break\n#         break","metadata":{"execution":{"iopub.status.busy":"2023-12-12T11:47:15.501534Z","iopub.execute_input":"2023-12-12T11:47:15.502420Z","iopub.status.idle":"2023-12-12T11:47:15.628851Z","shell.execute_reply.started":"2023-12-12T11:47:15.502385Z","shell.execute_reply":"2023-12-12T11:47:15.627848Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Real:  Вопрос: что на изображении? Ответ: Два высоких жирафа гуляют по своей зоовыставке.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\nGenerated:  ['Два жира жирафа,ляют по полю территорииозащитставке.']\nBleu: 64.31870218238024\n","output_type":"stream"}]},{"cell_type":"code","source":"#!ls /kaggle/working/checkpoints","metadata":{"execution":{"iopub.status.busy":"2023-12-12T11:38:28.586370Z","iopub.execute_input":"2023-12-12T11:38:28.587219Z","iopub.status.idle":"2023-12-12T11:38:29.687918Z","shell.execute_reply.started":"2023-12-12T11:38:28.587182Z","shell.execute_reply":"2023-12-12T11:38:29.686661Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"20bs_adamw-001_gpt2_medium.pt  20bs_adamw_latest_gpt2_medium.pt\n","output_type":"stream"}]},{"cell_type":"code","source":"#!rm /kaggle/working/first_start-003_gpt2_medium.pt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!mv /kaggle/working/checkpoints/20bs_adamw-001_gpt2_medium.pt /kaggle/working/\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T11:38:45.239475Z","iopub.execute_input":"2023-12-12T11:38:45.240003Z","iopub.status.idle":"2023-12-12T11:38:46.387130Z","shell.execute_reply.started":"2023-12-12T11:38:45.239923Z","shell.execute_reply":"2023-12-12T11:38:46.385768Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"#!zip weights.zip 20bs_adamw-001_gpt2_medium.pt","metadata":{"execution":{"iopub.status.busy":"2023-12-12T11:39:16.005672Z","iopub.execute_input":"2023-12-12T11:39:16.006586Z","iopub.status.idle":"2023-12-12T11:45:59.255270Z","shell.execute_reply.started":"2023-12-12T11:39:16.006547Z","shell.execute_reply":"2023-12-12T11:45:59.254041Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"  adding: 20bs_adamw-001_gpt2_medium.pt (deflated 22%)\n","output_type":"stream"}]},{"cell_type":"code","source":"question_tokens = torch.tensor(train_dataset.tokenizer.encode(\"Вопрос: что на изображении? Ответ:\"), dtype=torch.int64)\nquestion_tokens = torch.cat((question_tokens, torch.zeros(train_dataset.max_seq_len - question_tokens.shape[0], dtype=torch.int64) - 1))\nq_mask = question_tokens.ge(0)\nquestion_tokens[~q_mask] = 0\nq_mask = q_mask.float()\nq_mask = torch.cat((torch.ones(train_dataset.prefix_length), q_mask), dim=0)\nquestion_tokens = torch.stack([question_tokens] * args.bs).to(device)\nq_mask = torch.stack([q_mask] * args.bs).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:24:20.955702Z","iopub.execute_input":"2023-12-12T17:24:20.956140Z","iopub.status.idle":"2023-12-12T17:24:20.995400Z","shell.execute_reply.started":"2023-12-12T17:24:20.956107Z","shell.execute_reply":"2023-12-12T17:24:20.994507Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"unsup_train_dataloader = DataLoader(\n    unsup_train_dataset,\n    batch_size=args.bs,\n    shuffle=True,\n    drop_last=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:24:20.997186Z","iopub.execute_input":"2023-12-12T17:24:20.997480Z","iopub.status.idle":"2023-12-12T17:24:21.002833Z","shell.execute_reply.started":"2023-12-12T17:24:20.997454Z","shell.execute_reply":"2023-12-12T17:24:21.001866Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"unsup_optimizer = AdamW(\n        model.parameters(),\n        lr=args.lr,\n    )\n\nunsup_scheduler = get_linear_schedule_with_warmup(\n    unsup_optimizer,\n    num_warmup_steps=args.warmup_steps,\n    num_training_steps=args.epochs * len(unsup_train_dataloader)\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:24:21.004909Z","iopub.execute_input":"2023-12-12T17:24:21.005291Z","iopub.status.idle":"2023-12-12T17:24:21.022411Z","shell.execute_reply.started":"2023-12-12T17:24:21.005254Z","shell.execute_reply":"2023-12-12T17:24:21.021288Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\ndef unsup_train(\n    train_dataset: ClipCocoDataset,\n    train_dataloader,\n    #valid_dataset: ClipCocoDataset,\n    model: ClipCaptionModel,\n    optimizer,\n    scheduler,\n    args,\n    clip,\n    processor_clip,\n    warmup_steps: int = 5000,\n    output_dir: str = \".\",\n    output_prefix: str = \"\"\n):\n    #\n    batch_size = args.bs\n    epochs = args.epochs\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    model = freeze(model)\n\n\n    model.train()\n\n    mean_epoch_train_loss = []\n    mean_bleu_train_epoch = []\n    \n    \n    for epoch in range(epochs):\n        loss_train_epoch = []\n        bleu_train_epoch = []\n        print(f\">>> Training epoch {epoch+1}\")\n        sys.stdout.flush()\n        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n        step=0\n        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n            model.zero_grad()\n            step += 1\n            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.bfloat16)\n            \n            outputs = model(question_tokens, prefix, q_mask)\n            \n            logits = outputs.logits[:, train_dataset.prefix_length - 1: -1]\n            \n            generated_texts = []\n    \n            with torch.no_grad():\n                for b in range(batch_size):\n                    generated_text_batch = train_dataset.tokenizer.decode(logits[b].argmax(dim=-1).tolist())\n                    first_dot_index = generated_text_batch.find('.')\n                    if first_dot_index != -1:\n                        generated_texts.append(generated_text_batch[35:first_dot_index + 1])\n                    else:\n                        generated_texts.append(generated_text_batch[35:])\n                    \n                inputs = processor(text=generated_texts, images=None, return_tensors='pt', padding=True)\n                embedding_text = clip.encode_text(inputs['input_ids'].to(device))\n            \n            \n            \n            #last_hiddens_state = outputs.hidden_states[-1][:, :model.prefix_length]\n            #image_emb = model.clip_project(prefix.float()).view(-1, model.prefix_length, model.gpt_embedding_size)\n            \n            loss = - torch.mean(F.cosine_similarity(embedding_text, prefix, dim=-1))\n\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            optimizer.zero_grad()\n            progress.set_postfix({\"unsup_loss_train\": loss.item()})\n            \n\n            loss_train_epoch.append(loss.item())\n\n            if step % 500 == 0:\n                wandb.log({\"unsup_loss_train\":  loss.item()})\n            \n            if step % 1000 == 0:\n                with torch.no_grad():\n                    # BLEU-4\n                    #logits = outputs.logits[:, train_dataset.prefix_length - 1: -1]\n                    logits_cpu = logits.cpu()\n                    tokens_cpu = tokens.cpu()\n                    #generated_texts = []\n                    real_text = []\n                    for b in range(batch_size):\n                        #generated_text_batch = train_dataset.tokenizer.decode(logits_cpu[b].argmax(dim=-1).tolist())\n                        #first_dot_index = generated_text_batch.find('.')\n                        #if first_dot_index != -1:\n                        #    generated_texts.append(generated_text_batch[35:first_dot_index + 1])\n                        #else:\n                        #    generated_texts.append(generated_text_batch[35:])\n                        #\n                        real_text_batch = train_dataset.tokenizer.decode(tokens_cpu[b].tolist())\n                        first_pad_index = real_text_batch.find('<pad>')\n                        if first_pad_index != -1:\n                            real_text.append(real_text_batch[35:first_pad_index])\n                        else:\n                            real_text.append(real_text_batch[35:])\n                    \n                    bleu = calc_bleu(generated_texts, real_text)\n                    wandb.log({\"bleu-4 unsupervised train\":  bleu})\n                    bleu_train_epoch.append(bleu)\n                    \n\n            progress.update()\n            if (idx + 1) % 7000 == 0:\n                torch.save(\n                    model.state_dict(),\n                    os.path.join(output_dir, f\"{output_prefix}_latest_gpt2_medium.pt\"),\n                )\n        progress.close()\n        if epoch % args.save_every == 0:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}-{(epoch+1):03d}_gpt2_medium.pt\"),\n            )\n        mean_epoch_train_loss.append(np.mean(loss_train_epoch))\n        mean_bleu_train_epoch.append(np.mean(bleu_train_epoch))\n        \n        wandb.log({\"mean_epoch_unsup_train_loss\": mean_epoch_train_loss[-1]})\n        wandb.log({\"mean_bleu_unsup_train_epoch\": mean_bleu_train_epoch[-1]})\n\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:24:21.024280Z","iopub.execute_input":"2023-12-12T17:24:21.024657Z","iopub.status.idle":"2023-12-12T17:24:21.051620Z","shell.execute_reply.started":"2023-12-12T17:24:21.024619Z","shell.execute_reply":"2023-12-12T17:24:21.050672Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"import torch\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:24:21.052753Z","iopub.execute_input":"2023-12-12T17:24:21.054625Z","iopub.status.idle":"2023-12-12T17:24:21.840318Z","shell.execute_reply.started":"2023-12-12T17:24:21.054580Z","shell.execute_reply":"2023-12-12T17:24:21.839194Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"import ruclip\n\n\nru_clip, ru_processor = ruclip.load('ruclip-vit-base-patch32-384', device=device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train both prefix and GPT2\")\nsys.stdout.flush()\nmodel = unsup_train(\n    unsup_train_dataset,\n    unsup_train_dataloader,\n    model,\n    unsup_optimizer,\n    unsup_scheduler,\n    args,\n    clip = ru_clip,\n    processor_clip = ru_processor,\n    warmup_steps=args.warmup_steps,\n    output_dir=args.out_dir,\n    output_prefix=args.prefix\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:24:21.841954Z","iopub.execute_input":"2023-12-12T17:24:21.842334Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Train both prefix and GPT2\n>>> Training epoch 1\n","output_type":"stream"},{"name":"stderr","text":"20bs_adamw:   4%|▍         | 3769/96626 [17:50<7:24:19,  3.48it/s, unsup_loss_train=-.963]","output_type":"stream"}]},{"cell_type":"code","source":"with torch.no_grad():\n    for idx, (tokens, mask, prefix) in enumerate(unsup_train_dataloader):\n        model.zero_grad()\n        tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.bfloat16)\n\n        outputs = model(question_tokens, prefix, q_mask)\n        logits = outputs.logits[:, train_dataset.prefix_length - 1: -1]\n        \n        # BLEU-4\n        logits_cpu = logits.cpu()\n        tokens_cpu = tokens.cpu()\n        generated_texts = []\n        real_text = []\n        for b in range(args.bs):\n            generated_text_batch = train_dataset.tokenizer.decode(logits_cpu[b].argmax(dim=-1).tolist())\n            first_dot_index = generated_text_batch.find('.')\n            if first_dot_index != -1:\n                generated_texts.append(generated_text_batch[35:first_dot_index + 1])\n            else:\n                generated_texts.append(generated_text_batch[35:])\n\n            real_text_batch = train_dataset.tokenizer.decode(tokens_cpu[b].tolist())\n            first_pad_index = real_text_batch.find('<pad>')\n            if first_pad_index != -1:\n                real_text.append(real_text_batch[35:first_pad_index])\n            else:\n                real_text.append(real_text_batch[35:])\n                \n            print('Real: ', real_text_batch)\n            print('Generated: ', generated_texts)\n            print('Bleu:', calc_bleu(generated_texts, real_text))\n            break\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(\n    model.state_dict(),\n    os.path.join('checkpoints', \"last_weights.pt\"),\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/checkpoints","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm /kaggle/working/last_weights.pt\n!rm /kaggle/working/weights.zip","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv /kaggle/working/checkpoints/last_weights.pt /kaggle/working/\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip weights.zip last_weights.pt","metadata":{},"execution_count":null,"outputs":[]}]}