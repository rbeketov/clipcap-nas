{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7194192,"sourceType":"datasetVersion","datasetId":4160414},{"sourceId":7210860,"sourceType":"datasetVersion","datasetId":4172391},{"sourceId":7218864,"sourceType":"datasetVersion","datasetId":4178048},{"sourceId":7223268,"sourceType":"datasetVersion","datasetId":4181119},{"sourceId":7225443,"sourceType":"datasetVersion","datasetId":4182735},{"sourceId":7228559,"sourceType":"datasetVersion","datasetId":4184925}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wandb\n#!pip install bitsandbytes\n#!pip install ruclip==0.0.2\n!pip install transformers==4.27.4\n#!pip install pycocotools\n#!pip install git+https://github.com/openai/CLIP.git\n#!pip install open_clip_torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-18T09:09:55.554482Z","iopub.execute_input":"2023-12-18T09:09:55.554743Z","iopub.status.idle":"2023-12-18T09:10:29.473866Z","shell.execute_reply.started":"2023-12-18T09:09:55.554719Z","shell.execute_reply":"2023-12-18T09:10:29.472623Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.1)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.32)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.39.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.1.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nCollecting transformers==4.27.4\n  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (2.31.0)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.4)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.4) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.4) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.4) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.27.4) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.4) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.4) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.4) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.4) (2023.11.17)\nInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.0\n    Uninstalling tokenizers-0.15.0:\n      Successfully uninstalled tokenizers-0.15.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.36.0\n    Uninstalling transformers-4.36.0:\n      Successfully uninstalled transformers-4.36.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.27.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.27.4\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport os\nimport pickle\nimport sys\nimport argparse\nimport json\n\nimport random\nimport io\n\nimport wandb\nimport nltk\nimport numpy as np\n\nfrom nltk.translate.bleu_score import corpus_bleu\n\nfrom sklearn.model_selection import train_test_split\n\nfrom datasets import load_dataset, load_metric\n\nfrom torch.nn import functional as nnf\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast\nfrom torch.utils.data import Subset\n\n\nfrom transformers import GPT2Config, GPT2Model\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers.optimization import Adafactor, AdafactorSchedule\n\nfrom typing import Tuple, Optional, Union\nfrom tqdm import tqdm, trange\nfrom enum import Enum","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:29.476168Z","iopub.execute_input":"2023-12-18T09:10:29.476565Z","iopub.status.idle":"2023-12-18T09:10:46.536268Z","shell.execute_reply.started":"2023-12-18T09:10:29.476527Z","shell.execute_reply":"2023-12-18T09:10:46.535509Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"manualSeed = 1337\n#manualSeed = random.randint(1, 10000) # use if you want new results\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:46.537495Z","iopub.execute_input":"2023-12-18T09:10:46.538219Z","iopub.status.idle":"2023-12-18T09:10:46.573450Z","shell.execute_reply.started":"2023-12-18T09:10:46.538189Z","shell.execute_reply":"2023-12-18T09:10:46.572767Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class MlpTransformer(nn.Module):\n    def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):\n        super().__init__()\n        out_d = out_d if out_d is not None else in_dim\n        self.fc1 = nn.Linear(in_dim, h_dim)\n        self.act = act\n        self.fc2 = nn.Linear(h_dim, out_d)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:46.576099Z","iopub.execute_input":"2023-12-18T09:10:46.576710Z","iopub.status.idle":"2023-12-18T09:10:46.586327Z","shell.execute_reply.started":"2023-12-18T09:10:46.576674Z","shell.execute_reply":"2023-12-18T09:10:46.585317Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n\n    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim_self // num_heads\n        self.scale = head_dim ** -0.5\n        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n        self.project = nn.Linear(dim_self, dim_self)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, y=None, mask=None):\n        y = y if y is not None else x\n        b, n, c = x.shape\n        _, m, d = y.shape\n        # b n h dh\n        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n        # b m 2 h dh\n        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n        if mask is not None:\n            if mask.dim() == 2:\n                mask = mask.unsqueeze(1)\n            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n        attention = attention.softmax(dim=2)\n        \n        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n        out = self.project(out)\n        return out, attention","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:46.587514Z","iopub.execute_input":"2023-12-18T09:10:46.587861Z","iopub.status.idle":"2023-12-18T09:10:46.599735Z","shell.execute_reply.started":"2023-12-18T09:10:46.587827Z","shell.execute_reply":"2023-12-18T09:10:46.598953Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class TransformerLayer(nn.Module):\n\n    def forward_with_attention(self, x, y=None, mask=None):\n        x_, attention = self.attn(self.norm1(x), y, mask)\n        x = x + x_\n        x = x + self.mlp(self.norm2(x))\n        return x, attention\n\n    def forward(self, x, y=None, mask=None):\n        x = x + self.attn(self.norm1(x), y, mask)[0]\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n                 norm_layer: nn.Module = nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim_self)\n        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n        self.norm2 = norm_layer(dim_self)\n        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n\n\nclass Transformer(nn.Module):\n\n    def forward_with_attention(self, x, y=None, mask=None):\n        attentions = []\n        for layer in self.layers:\n            x, att = layer.forward_with_attention(x, y, mask)\n            attentions.append(att)\n        return x, attentions\n\n    def forward(self, x, y=None, mask=None):\n        for i, layer in enumerate(self.layers):\n            if i % 2 == 0 and self.enc_dec: # cross\n                x = layer(x, y)\n            elif self.enc_dec:  # self\n                x = layer(x, x, mask)\n            else:  # self or cross\n                x = layer(x, y, mask)\n        return x\n\n    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,\n                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):\n        super(Transformer, self).__init__()\n        dim_ref = dim_ref if dim_ref is not None else dim_self\n        self.enc_dec = enc_dec\n        if enc_dec:\n            num_layers = num_layers * 2\n        layers = []\n        for i in range(num_layers):\n            if i % 2 == 0 and enc_dec:  # cross\n                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n            elif enc_dec:  # self\n                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n            else:  # self or cross\n                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n        self.layers = nn.ModuleList(layers)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:46.600852Z","iopub.execute_input":"2023-12-18T09:10:46.601101Z","iopub.status.idle":"2023-12-18T09:10:46.616636Z","shell.execute_reply.started":"2023-12-18T09:10:46.601079Z","shell.execute_reply":"2023-12-18T09:10:46.615867Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class TransformerMapper(nn.Module):\n\n    def forward(self, x):\n        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n        prefix = torch.cat((x, prefix), dim=1)\n        out = self.transformer(prefix)[:, self.clip_length:]\n        return out\n\n    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n        super(TransformerMapper, self).__init__()\n        self.clip_length = clip_length\n        self.transformer = Transformer(dim_embedding, 8, num_layers)\n        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:46.617534Z","iopub.execute_input":"2023-12-18T09:10:46.617787Z","iopub.status.idle":"2023-12-18T09:10:46.631261Z","shell.execute_reply.started":"2023-12-18T09:10:46.617765Z","shell.execute_reply":"2023-12-18T09:10:46.630358Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\ndef freeze(\n    model,\n    freeze_emb=False,\n    freeze_ln=True,\n    freeze_attn=True,\n    freeze_ff=True,\n    freeze_other=False,\n):\n    \n    for name, p in model.named_parameters():\n    # freeze all parameters except the layernorm and positional embeddings\n        name = name.lower()\n        if 'ln' in name or 'norm' in name:\n            p.requires_grad = not freeze_ln\n        elif 'embeddings' in name:\n            p.requires_grad = not freeze_emb\n        elif 'mlp' in name:\n            p.requires_grad = not freeze_ff\n        elif 'attn' in name:\n            p.requires_grad = not freeze_attn\n        else:\n            p.requires_grad = not freeze_other\n           \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:46.632411Z","iopub.execute_input":"2023-12-18T09:10:46.632760Z","iopub.status.idle":"2023-12-18T09:10:46.645199Z","shell.execute_reply.started":"2023-12-18T09:10:46.632730Z","shell.execute_reply":"2023-12-18T09:10:46.644506Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from enum import Enum\nclass MappingType(Enum):\n    MLP = 'mlp'\n    Transformer = 'transformer'","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:46.646147Z","iopub.execute_input":"2023-12-18T09:10:46.646413Z","iopub.status.idle":"2023-12-18T09:10:46.656049Z","shell.execute_reply.started":"2023-12-18T09:10:46.646390Z","shell.execute_reply":"2023-12-18T09:10:46.655257Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"gpt_model_name = 'sberbank-ai/rugpt3medium_based_on_gpt2'\nclass ClipCaptionModel(nn.Module):\n    def __init__(\n        self,\n        prefix_length: int,\n        clip_length: Optional[int] = 10,\n        prefix_size: int = 512,\n        num_layers: int = 8,\n        mapping_type: MappingType = MappingType.Transformer\n    ):\n        super(ClipCaptionModel, self).__init__()\n        self.prefix_length = prefix_length\n\n        self.gpt = GPT2LMHeadModel.from_pretrained(gpt_model_name)\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n\n        if mapping_type == MappingType.MLP:\n            self.clip_project = MLP((\n                prefix_size,\n                self.gpt_embedding_size * prefix_length // 2,\n                self.gpt_embedding_size * prefix_length\n            ))\n        else:\n            self.clip_project = TransformerMapper(\n                prefix_size,                 #512\n                self.gpt_embedding_size,    #1024\n                prefix_length,              #30\n                clip_length,               #10\n                num_layers                 #8\n            )\n#             self.gpt_project = TransformerMapper(\n#                 65536,               \n#                 self.gpt_embedding_size,    \n#                 prefix_length,           \n#                 1,               \n#                 num_layers             \n#             )\n            self.gpt_project = TransformerMapper(\n                65536,               \n                prefix_size,    \n                1,           \n                1,               \n                num_layers             \n            ).to(device)\n        \n    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n    \n    @autocast() \n    def forward(\n        self,        \n        tokens: torch.Tensor,\n        prefix: torch.Tensor,\n        mask: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None\n    ):\n        embedding_text = self.gpt.transformer.wte(tokens)\n        prefix_projections = self.clip_project(\n            prefix.float()\n        ).view(-1, self.prefix_length, self.gpt_embedding_size)\n\n        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n\n        if labels is not None:\n            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n            labels = torch.cat((dummy_token, tokens), dim=1)\n        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask, output_hidden_states = True)\n        \n        return out\n\nclass ClipCaptionPrefix(ClipCaptionModel):\n    def parameters(self, recurse: bool = True):\n        return self.clip_project.parameters()\n\n    def train(self, mode: bool = True):\n        super(ClipCaptionPrefix, self).train(mode)\n        self.gpt.eval()\n        return self","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:46.659919Z","iopub.execute_input":"2023-12-18T09:10:46.660219Z","iopub.status.idle":"2023-12-18T09:10:46.673690Z","shell.execute_reply.started":"2023-12-18T09:10:46.660197Z","shell.execute_reply":"2023-12-18T09:10:46.672851Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class CPU_Unpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        if module == 'torch.storage' and name == '_load_from_bytes':\n            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n        else: return super().find_class(module, name)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:46.674628Z","iopub.execute_input":"2023-12-18T09:10:46.674882Z","iopub.status.idle":"2023-12-18T09:10:46.687103Z","shell.execute_reply.started":"2023-12-18T09:10:46.674859Z","shell.execute_reply":"2023-12-18T09:10:46.686378Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class ClipCocoDataset(Dataset):\n    def __init__(\n        self,\n        data_path: str,\n        prefix_length=30,\n        model_type = gpt_model_name,\n        normalize_prefix=False,\n        train=True,\n    ):\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n        self.prefix_length = prefix_length\n        self.normalize_prefix = normalize_prefix\n        if train:\n            with open(data_path, 'rb') as f:\n                all_data = CPU_Unpickler(f).load() #pickle.load(f)\n            print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n        else:\n            with open(data_path, 'rb') as f:\n                all_data = CPU_Unpickler(f).load() #pickle.load(f)\n            print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n\n        sys.stdout.flush()\n        self.prefixes = all_data[\"clip_embedding\"]\n        captions_raw = all_data[\"captions\"]\n        \n        self.captions = captions_raw\n\n        self.image_id = all_data[\"path_images\"]\n\n        self.captions_tokens = []\n        self.caption2embedding = []\n        max_seq_len = 0\n        i = 0\n        for caption in tqdm(captions_raw):\n            self.captions_tokens.append(\n                torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64)\n            )\n            self.caption2embedding.append(self.prefixes[i])\n            i += 1\n            max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n\n        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n\n    def get_image(self, item):\n        if self.train:\n            path_img = f\"/kaggle/input/train2014/train2014/{self.image_id[item]}\"\n        else:\n            path_img = f\"/kaggle/input/val2014/val2014/{self.image_id[item]}\"\n            \n        image = cv2.imread(path_img)\n        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        image.thumbnail((196, 196), Image.Resampling.LANCZOS)\n        return image\n    \n    def pad_tokens(self, item: int):\n        tokens = self.captions_tokens[item]\n        padding = self.max_seq_len - tokens.shape[0]\n        if padding > 0:\n            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            #self.captions_tokens[item] = tokens\n        elif padding < 0:\n            tokens = tokens[:self.max_seq_len]\n            #self.captions_tokens[item] = tokens\n        mask = tokens.ge(0)  # mask is zero where we out of sequence\n        tokens[~mask] = 0\n        mask = mask.float()\n        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n        return tokens, mask\n    \n    def __len__(self) -> int:\n        return len(self.captions_tokens)\n\n    def __getitem__(self, item):\n        tokens, mask = self.pad_tokens(item)\n        prefix = self.prefixes[item]\n        if self.normalize_prefix:\n            prefix = prefix.float()\n            prefix = prefix / prefix.norm(2, -1)\n        return tokens, mask, prefix","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:46.688155Z","iopub.execute_input":"2023-12-18T09:10:46.688424Z","iopub.status.idle":"2023-12-18T09:10:46.705054Z","shell.execute_reply.started":"2023-12-18T09:10:46.688400Z","shell.execute_reply":"2023-12-18T09:10:46.704136Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def calc_bleu(y_pred, y_true):\n    references = []\n    hypotheses = [] \n    for p, t in zip(y_pred, y_true):\n        hypotheses.append(p.split())\n        references.append([t.split()])\n    # Рассчитываем BLEU-4\n    bleu_score = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n    return bleu_score*100\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:46.706161Z","iopub.execute_input":"2023-12-18T09:10:46.706859Z","iopub.status.idle":"2023-12-18T09:10:46.718637Z","shell.execute_reply.started":"2023-12-18T09:10:46.706826Z","shell.execute_reply":"2023-12-18T09:10:46.717856Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## **TRAIN LOOP**","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\nimport gc\n\ndef train(\n    train_dataset: ClipCocoDataset,\n    train_dataloader,\n    model: ClipCaptionModel,\n    optimizer,\n    scheduler,\n    args,\n    warmup_steps: int = 5000,\n    output_dir: str = \".\",\n    output_prefix: str = \"\",   \n):\n    \n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    batch_size = args.bs\n    epochs = args.epochs\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n\n\n\n    model.train()\n    mean_epoch_train_loss = []\n    mean_bleu_train_epoch = []\n    \n    \n\n    \n    for epoch in range(epochs):\n        loss_train_epoch = []\n        bleu_train_epoch = []\n        print(f\">>> Training epoch {epoch+1}\")\n        sys.stdout.flush()\n        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n        step=0\n        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n            model.zero_grad()\n            step += 1\n            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.bfloat16)\n            \n            #outputs = model(question_tokens, prefix, q_mask)\n            outputs = model(tokens, prefix, mask)\n            last_hiddens_state = outputs.hidden_states[-1][:, train_dataset.prefix_length - 1: -1]\n            logits = outputs.logits[:, train_dataset.prefix_length - 1: -1]\n            \n#             with torch.no_grad():\n#                 image_emb = model.clip_project(prefix.float()).view(-1, model.prefix_length, model.gpt_embedding_size)\n            image_emb = prefix.float()\n    \n#             cross_loss = nnf.cross_entropy(\n#                  logits.reshape(-1, logits.shape[-1]),\n#                  tokens.flatten().to(torch.int64),\n#                  ignore_index=0\n#              )\n            \n            text_emb = model.gpt_project(last_hiddens_state.flatten(start_dim = 1))\n            cosine_loss = torch.mean(1 - F.cosine_similarity(image_emb,\n                                                   text_emb, dim=-1))\n            #loss = cross_loss + cosine_loss \n            loss = cosine_loss \n            \n#             last_hiddens_state = outputs.hidden_states[-1][:, :model.prefix_length]\n            \n#             with torch.no_grad():\n#                 image_emb = model.clip_project(prefix.float()).view(-1, model.prefix_length, model.gpt_embedding_size)\n            \n#             cosine_loss = torch.mean(1 - F.cosine_similarity(image_emb,\n#                                                    last_hiddens_state, dim=-1))\n#             loss = cosine_loss + cross_loss\n            \n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n            loss_train_epoch.append(loss.item())\n\n            progress.set_postfix({\"selfsupervised_loss_train\": loss.item()})\n    \n            if step % 5 == 0:\n                wandb.log({\"selfsupervised_loss_train\": loss.item()})\n                wandb.log({\"mean_selfsupervised_loss_train\":  np.mean(loss_train_epoch)})\n            \n            if step % 10 == 0:\n                with torch.no_grad():\n                    # BLEU-4\n                    logits_cpu = logits.cpu()\n                    tokens_cpu = tokens.cpu()\n                    generated_texts = []\n                    real_text = []\n                    for b in range(batch_size):\n                        generated_text_batch = train_dataset.tokenizer.decode(logits_cpu[b].argmax(dim=-1).tolist())\n                        first_dot_index = generated_text_batch.find('.')\n                        if first_dot_index != -1:\n                            generated_texts.append(generated_text_batch[35:first_dot_index + 1])\n                        else:\n                            generated_texts.append(generated_text_batch[35:])\n                        \n                        real_text_batch = train_dataset.tokenizer.decode(tokens_cpu[b].tolist())\n                        first_pad_index = real_text_batch.find('<pad>')\n                        if first_pad_index != -1:\n                            real_text.append(real_text_batch[35:first_pad_index])\n                        else:\n                            real_text.append(real_text_batch[35:])\n                    \n                    bleu = calc_bleu(generated_texts, real_text)\n                    wandb.log({\"selfsupervised_bleu-4_train\":  bleu})\n                    bleu_train_epoch.append(bleu)\n                    wandb.log({\"mean_selfsupervised_bleu-4_train\": np.mean(bleu_train_epoch)})\n\n            progress.update()\n            if (idx + 1) % 7000 == 0:\n                torch.save(\n                    model.state_dict(),\n                    os.path.join(output_dir, f\"{output_prefix}_latest_gpt2_medium.pt\"),\n                )\n        progress.close()\n        if epoch % args.save_every == 0:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}-{(epoch+1):03d}_gpt2_medium.pt\"),\n            )\n        mean_epoch_train_loss.append(np.mean(loss_train_epoch))\n        mean_bleu_train_epoch.append(np.mean(bleu_train_epoch))\n        \n        wandb.log({\"mean_epoch_selfsup_train_loss\": mean_epoch_train_loss[-1]})\n        wandb.log({\"mean_bleu_selfsup_train_epoch\": mean_bleu_train_epoch[-1]})\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:46.720908Z","iopub.execute_input":"2023-12-18T09:10:46.721165Z","iopub.status.idle":"2023-12-18T09:10:46.742306Z","shell.execute_reply.started":"2023-12-18T09:10:46.721143Z","shell.execute_reply":"2023-12-18T09:10:46.741473Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# c5ae028d4beaea1eeb875f10d0f8ba4f31e45135","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:46.743377Z","iopub.execute_input":"2023-12-18T09:10:46.743680Z","iopub.status.idle":"2023-12-18T09:10:46.756398Z","shell.execute_reply.started":"2023-12-18T09:10:46.743657Z","shell.execute_reply":"2023-12-18T09:10:46.755566Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"wandb.login(key='1cdb337eaa53f9f6d40e35141bacd4a8bf168e38')","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:46.757521Z","iopub.execute_input":"2023-12-18T09:10:46.757784Z","iopub.status.idle":"2023-12-18T09:10:49.656269Z","shell.execute_reply.started":"2023-12-18T09:10:46.757762Z","shell.execute_reply":"2023-12-18T09:10:49.655422Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"wandb.init(project=\"ClipCap_NAS\", name=\"selftrain\")","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:10:49.657492Z","iopub.execute_input":"2023-12-18T09:10:49.658641Z","iopub.status.idle":"2023-12-18T09:11:21.096544Z","shell.execute_reply.started":"2023-12-18T09:10:49.658604Z","shell.execute_reply":"2023-12-18T09:11:21.095683Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msanchoues\u001b[0m (\u001b[33mclipcap_nas\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231218_091049-6s5bz6ab</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/clipcap_nas/ClipCap_NAS/runs/6s5bz6ab' target=\"_blank\">selftrain</a></strong> to <a href='https://wandb.ai/clipcap_nas/ClipCap_NAS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/clipcap_nas/ClipCap_NAS' target=\"_blank\">https://wandb.ai/clipcap_nas/ClipCap_NAS</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/clipcap_nas/ClipCap_NAS/runs/6s5bz6ab' target=\"_blank\">https://wandb.ai/clipcap_nas/ClipCap_NAS/runs/6s5bz6ab</a>"},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/clipcap_nas/ClipCap_NAS/runs/6s5bz6ab?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x78d033d25e70>"},"metadata":{}}]},{"cell_type":"code","source":"class Args():\n    def __init__(self):\n        self.backbone = gpt_model_name\n        self.train_data = \"/kaggle/input/coco2014-ru-clip-embeddings/embeddings_ru_clip_train.pkl\"\n        self.valid_data = \"/kaggle/input/coco2014-ru-clip-embeddings/embeddings_ru_clip_valid.pkl\"\n        self.out_dir = 'checkpoints'\n        self.prefix = 'transformer_gpt'\n        self.epochs = 1\n        self.save_every = 1\n        self.prefix_length = 30\n        self.bs = 20\n        self.only_prefix = False\n        self.lr = 2e-5\n        self.warmup_steps = 5000\nargs = Args()","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:11:21.097731Z","iopub.execute_input":"2023-12-18T09:11:21.098008Z","iopub.status.idle":"2023-12-18T09:11:21.104725Z","shell.execute_reply.started":"2023-12-18T09:11:21.097981Z","shell.execute_reply":"2023-12-18T09:11:21.103871Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_dataset = ClipCocoDataset(args.train_data, args.prefix_length, train=True)\n#train_dataset = ClipCocoDataset(args.valid_data, args.prefix_length, train=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:11:21.105977Z","iopub.execute_input":"2023-12-18T09:11:21.106902Z","iopub.status.idle":"2023-12-18T09:13:46.961181Z","shell.execute_reply.started":"2023-12-18T09:11:21.106860Z","shell.execute_reply":"2023-12-18T09:13:46.959933Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.61M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d287fefe11944c559be14cca845cad2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52f741732b9f4ac2962871590c4775c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/574 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85e537455d594fe8a21968a5e5248a0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3eb01a90165450e972874d65dec5ed8"}},"metadata":{}},{"name":"stdout","text":"Data size is 414113\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 414113/414113 [02:10<00:00, 3180.44it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"question_tokens = torch.tensor(train_dataset.tokenizer.encode(\"Вопрос: что на изображении? Ответ:\"), dtype=torch.int64)\nquestion_tokens = torch.cat((question_tokens, torch.zeros(train_dataset.max_seq_len - question_tokens.shape[0], dtype=torch.int64) - 1))\nq_mask = question_tokens.ge(0)\nquestion_tokens[~q_mask] = 0\nq_mask = q_mask.float()\nq_mask = torch.cat((torch.ones(train_dataset.prefix_length), q_mask), dim=0)\nquestion_tokens = torch.stack([question_tokens] * args.bs).to(device)\nq_mask = torch.stack([q_mask] * args.bs).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:13:46.962757Z","iopub.execute_input":"2023-12-18T09:13:46.963079Z","iopub.status.idle":"2023-12-18T09:13:51.959935Z","shell.execute_reply.started":"2023-12-18T09:13:46.963051Z","shell.execute_reply":"2023-12-18T09:13:51.958766Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"wandb.config = {\n  \"learning_rate\": args.lr,\n  \"epochs\": args.epochs,\n  \"batch_size\": args.bs\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:13:51.961671Z","iopub.execute_input":"2023-12-18T09:13:51.962066Z","iopub.status.idle":"2023-12-18T09:13:51.968719Z","shell.execute_reply.started":"2023-12-18T09:13:51.962029Z","shell.execute_reply":"2023-12-18T09:13:51.966899Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model = ClipCaptionModel(args.prefix_length)\nmodel_path = \"/kaggle/input/new-pretrained/checkpoints/superwised_weights.pt\"\nmodel.load_state_dict(torch.load(model_path, map_location='cpu'))\n\nfor p in model.clip_project.parameters():\n    p.requires_grad = False\n \nfor p in model.gpt_project.parameters():\n    p.requires_grad = True\n    \nfor p in model.gpt.parameters():\n    p.requires_grad = False\n    \nmodel = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\n\ndef sup_unsup_train_dataset(dataset, val_split):\n    sup_idx, unsup_idx = train_test_split(list(range(len(dataset))), test_size=1-val_split, shuffle=False)\n   # unsup_idx, _ = train_test_split(list(range(len(unsup_idx))), test_size=1-val_split, shuffle=False)\n   # return Subset(dataset, sup_idx), Subset(sup_idx, unsup_idx)\n    return Subset(dataset, sup_idx), Subset(dataset, unsup_idx)\n\nsup_train_dataset, unsup_train_dataset = sup_unsup_train_dataset(train_dataset, 0.5)\n\nsup_train_dataset.prefix_length = train_dataset.prefix_length\nunsup_train_dataset.prefix_length = train_dataset.prefix_length\nsup_train_dataset.tokenizer = train_dataset.tokenizer\nunsup_train_dataset.tokenizer = train_dataset.tokenizer\n\nprint(len(sup_train_dataset))\nprint(len(unsup_train_dataset))","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:06:38.621666Z","iopub.status.idle":"2023-12-18T09:06:38.622203Z","shell.execute_reply.started":"2023-12-18T09:06:38.621921Z","shell.execute_reply":"2023-12-18T09:06:38.621946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    unsup_train_dataset,\n    batch_size=args.bs,\n    shuffle=True,\n    drop_last=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:06:38.623449Z","iopub.status.idle":"2023-12-18T09:06:38.623962Z","shell.execute_reply.started":"2023-12-18T09:06:38.623685Z","shell.execute_reply":"2023-12-18T09:06:38.623726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(\n        model.parameters(),\n        lr=args.lr,\n    )\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=args.warmup_steps,\n    num_training_steps=args.epochs * len(train_dataloader)\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:06:38.624883Z","iopub.status.idle":"2023-12-18T09:06:38.625342Z","shell.execute_reply.started":"2023-12-18T09:06:38.625093Z","shell.execute_reply":"2023-12-18T09:06:38.625133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train(\n    unsup_train_dataset,\n    train_dataloader,\n    model,\n    optimizer,\n    scheduler,\n    args,\n    warmup_steps=args.warmup_steps,\n    output_dir=args.out_dir,\n    output_prefix=args.prefix\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:06:38.628983Z","iopub.status.idle":"2023-12-18T09:06:38.629478Z","shell.execute_reply.started":"2023-12-18T09:06:38.629260Z","shell.execute_reply":"2023-12-18T09:06:38.629281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n\n    for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n        model.zero_grad()\n        tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.bfloat16)\n\n        outputs = model(question_tokens, prefix, q_mask)\n\n        last_hiddens_state = outputs.hidden_states[-1][:, train_dataset.prefix_length - 1: -1]\n#         with torch.no_grad():\n#             image_emb = model.clip_project(prefix.float()).view(-1, model.prefix_length, model.gpt_embedding_size)\n\n        #loss = F.cosine_similarity(image_emb,\n        #                                       last_hiddens_state, dim=-1)\n                \n        logits = outputs.logits[:, train_dataset.prefix_length - 1: -1]\n        \n        generated_tokens = []\n        generated_texts = []\n#         print(gpt_project(last_hiddens_state.flatten(start_dim = 1)).shape)\n#         print(prefix.shape)\n        with torch.no_grad():\n            for b in range(args.bs):\n                generated_text_batch = train_dataset.tokenizer.decode(logits[b].argmax(dim=-1))\n                first_dot_index = generated_text_batch.find('.')\n                generated_texts.append(generated_text_batch)\n\n            #print(generated_texts)\n        break","metadata":{"execution":{"iopub.status.busy":"2023-12-18T07:12:13.383301Z","iopub.execute_input":"2023-12-18T07:12:13.384183Z","iopub.status.idle":"2023-12-18T07:12:13.718269Z","shell.execute_reply.started":"2023-12-18T07:12:13.384142Z","shell.execute_reply":"2023-12-18T07:12:13.717060Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"torch.Size([20, 1, 512])\ntorch.Size([20, 1, 512])\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(\n    model.state_dict(),\n    os.path.join(args.out_dir, \"superwised_weights.pt\"),\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T08:31:30.785855Z","iopub.execute_input":"2023-12-18T08:31:30.786238Z","iopub.status.idle":"2023-12-18T08:31:33.192614Z","shell.execute_reply.started":"2023-12-18T08:31:30.786207Z","shell.execute_reply":"2023-12-18T08:31:33.191443Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"!zip superwised_weights.zip checkpoints/superwised_weights.pt","metadata":{"execution":{"iopub.status.busy":"2023-12-18T08:31:41.404069Z","iopub.execute_input":"2023-12-18T08:31:41.405281Z","iopub.status.idle":"2023-12-18T08:33:27.132911Z","shell.execute_reply.started":"2023-12-18T08:31:41.405229Z","shell.execute_reply":"2023-12-18T08:33:27.131620Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"  adding: checkpoints/superwised_weights.pt (deflated 8%)\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls checkpoints","metadata":{"execution":{"iopub.status.busy":"2023-12-17T13:32:18.637585Z","iopub.execute_input":"2023-12-17T13:32:18.637979Z","iopub.status.idle":"2023-12-17T13:32:19.683055Z","shell.execute_reply.started":"2023-12-17T13:32:18.637949Z","shell.execute_reply":"2023-12-17T13:32:19.681934Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"semi_train.pt  transformer_gpt_latest_gpt2_medium.pt\n","output_type":"stream"}]},{"cell_type":"code","source":"!cd checkpoints\\semi_train_weights.zip semi_train_weights.zip","metadata":{},"execution_count":null,"outputs":[]}]}