{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":214432,"sourceType":"datasetVersion","datasetId":92290},{"sourceId":7174178,"sourceType":"datasetVersion","datasetId":4145480},{"sourceId":7174434,"sourceType":"datasetVersion","datasetId":4145682}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Переда запуском добавь коко2014 [датасет](https://www.kaggle.com/datasets/nadaibrahim/coco2014) в ноутбук)**","metadata":{}},{"cell_type":"code","source":"!pip install wandb\n!pip install bitsandbytes\n!pip install ruclip==0.0.2\n!pip install transformers==4.27.4\n!pip install pycocotools\n!pip install git+https://github.com/openai/CLIP.git\n!pip install open_clip_torch","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:04:56.417307Z","iopub.execute_input":"2023-12-11T12:04:56.417559Z","iopub.status.idle":"2023-12-11T12:06:58.643799Z","shell.execute_reply.started":"2023-12-11T12:04:56.417535Z","shell.execute_reply":"2023-12-11T12:06:58.642602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Качаем ембединги датасета","metadata":{}},{"cell_type":"code","source":"#!rm -rf train2014_emb.zip\n#!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=11OpNsXhmtafzItkGxaTqMXqgL2dQycl6' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=11OpNsXhmtafzItkGxaTqMXqgL2dQycl6\" -O train2014_emb.zip && rm -rf /tmp/cookies.txt\n#!unzip train2014_emb.zip\n\n#!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1a6kvugo_mA0qwd_anIxFXD4pzot5B16t' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1a6kvugo_mA0qwd_anIxFXD4pzot5B16t\" -O Features_train_coco_ru_vitb16_82783.pkl && rm -rf /tmp/cookies.txt\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:53:01.546372Z","iopub.execute_input":"2023-12-11T08:53:01.546708Z","iopub.status.idle":"2023-12-11T08:53:08.262248Z","shell.execute_reply.started":"2023-12-11T08:53:01.546679Z","shell.execute_reply":"2023-12-11T08:53:08.261120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!rm -rf val2014_emb.zip\n#!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1a-XuH0q5Ktlo6fIGFKQVkGEUkjiQ10X0' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1a-XuH0q5Ktlo6fIGFKQVkGEUkjiQ10X0\" -O val2014_emb.zip && rm -rf /tmp/cookies.txt\n#!unzip val2014_emb.zip\n\n\n#!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1qACM-Yg2nS-qBHUwzFY4Q7X8o5HlBgP7' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1qACM-Yg2nS-qBHUwzFY4Q7X8o5HlBgP7\" -O Features_val_coco_ru_vitb16.pkl && rm -rf /tmp/cookies.txt\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:53:33.172321Z","iopub.execute_input":"2023-12-11T08:53:33.173192Z","iopub.status.idle":"2023-12-11T08:53:34.736539Z","shell.execute_reply.started":"2023-12-11T08:53:33.173152Z","shell.execute_reply":"2023-12-11T08:53:34.735443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as nnf\nfrom torch.utils.data import Dataset, DataLoader\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:06:58.646205Z","iopub.execute_input":"2023-12-11T12:06:58.646599Z","iopub.status.idle":"2023-12-11T12:07:01.603254Z","shell.execute_reply.started":"2023-12-11T12:06:58.646562Z","shell.execute_reply":"2023-12-11T12:07:01.602359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm, trange\nimport os\nimport pickle\nimport sys\nimport argparse\nimport json\nfrom typing import Tuple, Optional, Union\nfrom torch.cuda.amp import autocast\n\nimport ruclip\nimport clip, open_clip\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:01.604774Z","iopub.execute_input":"2023-12-11T12:07:01.605276Z","iopub.status.idle":"2023-12-11T12:07:04.990835Z","shell.execute_reply.started":"2023-12-11T12:07:01.605240Z","shell.execute_reply":"2023-12-11T12:07:04.990047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Config, GPT2Model\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:04.993139Z","iopub.execute_input":"2023-12-11T12:07:04.993454Z","iopub.status.idle":"2023-12-11T12:07:05.224652Z","shell.execute_reply.started":"2023-12-11T12:07:04.993426Z","shell.execute_reply":"2023-12-11T12:07:05.223922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:35:01.393377Z","iopub.execute_input":"2023-12-11T09:35:01.393801Z","iopub.status.idle":"2023-12-11T09:35:11.258529Z","shell.execute_reply.started":"2023-12-11T09:35:01.393775Z","shell.execute_reply":"2023-12-11T09:35:11.257653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from multilingual_clip import pt_multilingual_clip","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:02:56.901652Z","iopub.execute_input":"2023-12-11T08:02:56.902242Z","iopub.status.idle":"2023-12-11T08:02:56.906613Z","shell.execute_reply.started":"2023-12-11T08:02:56.902211Z","shell.execute_reply":"2023-12-11T08:02:56.905556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"manualSeed = 1337\n#manualSeed = random.randint(1, 10000) # use if you want new results\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:05.225636Z","iopub.execute_input":"2023-12-11T12:07:05.226041Z","iopub.status.idle":"2023-12-11T12:07:05.234909Z","shell.execute_reply.started":"2023-12-11T12:07:05.226013Z","shell.execute_reply":"2023-12-11T12:07:05.234237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n        super(MLP, self).__init__()\n        layers = []\n        for i in range(len(sizes) - 1):\n            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n            if i < len(sizes) - 2:\n                layers.append(act())\n        self.model = nn.Sequential(*layers)\n    \n    @autocast()  \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:05.235947Z","iopub.execute_input":"2023-12-11T12:07:05.236248Z","iopub.status.idle":"2023-12-11T12:07:05.243648Z","shell.execute_reply.started":"2023-12-11T12:07:05.236224Z","shell.execute_reply":"2023-12-11T12:07:05.242719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def freeze(\n    model,\n    freeze_emb=False,\n    freeze_ln=False,\n    freeze_attn=True,\n    freeze_ff=True,\n    freeze_other=False,\n):\n    \n    for name, p in model.named_parameters():\n    # freeze all parameters except the layernorm and positional embeddings\n        name = name.lower()\n        if 'ln' in name or 'norm' in name:\n            p.requires_grad = not freeze_ln\n        elif 'embeddings' in name:\n            p.requires_grad = not freeze_emb\n        elif 'mlp' in name:\n            p.requires_grad = not freeze_ff\n        elif 'attn' in name:\n            p.requires_grad = not freeze_attn\n        else:\n            p.requires_grad = not freeze_other\n           \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:05.244981Z","iopub.execute_input":"2023-12-11T12:07:05.245282Z","iopub.status.idle":"2023-12-11T12:07:05.254778Z","shell.execute_reply.started":"2023-12-11T12:07:05.245258Z","shell.execute_reply":"2023-12-11T12:07:05.254030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpt_model_name = 'sberbank-ai/rugpt3medium_based_on_gpt2'\nclass ClipCaptionModel(nn.Module):\n    def __init__(self, prefix_length: int, prefix_size: int = 640):\n        super(ClipCaptionModel, self).__init__()\n        self.prefix_length = prefix_length\n\n        self.gpt = GPT2LMHeadModel.from_pretrained(gpt_model_name)\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n\n        self.clip_project = MLP((\n            prefix_size, \n            self.gpt_embedding_size * prefix_length // 2,\n            self.gpt_embedding_size * prefix_length\n        ))\n        \n    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n    \n    @autocast() \n    def forward(\n        self,        \n        tokens: torch.Tensor,\n        prefix: torch.Tensor,\n        mask: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None\n    ):\n        embedding_text = self.gpt.transformer.wte(tokens)\n        prefix_projections = self.clip_project(prefix.float()).view(-1, self.prefix_length, self.gpt_embedding_size)\n\n        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n        if labels is not None:\n            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n            labels = torch.cat((dummy_token, tokens), dim=1)\n        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask, output_hidden_states = True)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:05.255742Z","iopub.execute_input":"2023-12-11T12:07:05.256003Z","iopub.status.idle":"2023-12-11T12:07:05.271019Z","shell.execute_reply.started":"2023-12-11T12:07:05.255979Z","shell.execute_reply":"2023-12-11T12:07:05.270143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClipCaptionPrefix(ClipCaptionModel):\n    def parameters(self, recurse: bool = True):\n        return self.clip_project.parameters()\n\n    def train(self, mode: bool = True):\n        super(ClipCaptionPrefix, self).train(mode)\n        self.gpt.eval()\n        return self","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:05.272141Z","iopub.execute_input":"2023-12-11T12:07:05.272496Z","iopub.status.idle":"2023-12-11T12:07:05.285215Z","shell.execute_reply.started":"2023-12-11T12:07:05.272461Z","shell.execute_reply":"2023-12-11T12:07:05.284406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(gpt_model_name)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:05.288066Z","iopub.execute_input":"2023-12-11T12:07:05.288325Z","iopub.status.idle":"2023-12-11T12:07:07.565132Z","shell.execute_reply.started":"2023-12-11T12:07:05.288301Z","shell.execute_reply":"2023-12-11T12:07:07.564347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# запустится только с gpu, очень большая штука  --- LARGE же =)\n#clip_model, processor = ruclip.load(\"ruclip-vit-large-patch14-336\", device=\"cuda\")\n#clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16-plus-240', pretrained=\"laion400m_e32\")\n#clip_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:02:57.936747Z","iopub.execute_input":"2023-12-11T08:02:57.937429Z","iopub.status.idle":"2023-12-11T08:02:57.941716Z","shell.execute_reply.started":"2023-12-11T08:02:57.937394Z","shell.execute_reply":"2023-12-11T08:02:57.940743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Получение эмбедингов изображений","metadata":{}},{"cell_type":"code","source":"GET_EMBEDDING = False","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:02:57.943017Z","iopub.execute_input":"2023-12-11T08:02:57.943679Z","iopub.status.idle":"2023-12-11T08:02:57.955739Z","shell.execute_reply.started":"2023-12-11T08:02:57.943647Z","shell.execute_reply":"2023-12-11T08:02:57.954820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!rm -rf ru_capt.json\n\n#!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1uIO34T8d0ML23I30mcRFAD7niggm1kIt' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1uIO34T8d0ML23I30mcRFAD7niggm1kIt\" -O ru_capt.json && rm -rf /tmp/cookies.txt","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:02:16.601790Z","iopub.execute_input":"2023-12-11T08:02:16.602102Z","iopub.status.idle":"2023-12-11T08:02:18.564164Z","shell.execute_reply.started":"2023-12-11T08:02:16.602075Z","shell.execute_reply":"2023-12-11T08:02:18.563218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if GET_EMBEDDING:\n    import json\n    ru_capt = {}\n    with open('./ru_capt.json', 'r') as file:\n        for st in file:\n            data = json.loads(st)\n            ru_capt[data['id']] = data\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if GET_EMBEDDING:\n\n    import torchvision.datasets as dset\n    coco_train = dset.CocoDetection(\n        root = \"/kaggle/input/train2014/train2014\",\n        annFile = \"/kaggle/input/captions/annotations/captions_train2014.json\"\n    )\n    coco_val = dset.CocoDetection(\n        root = \"/kaggle/input/val2014/val2014\",\n        annFile = \"/kaggle/input/captions/annotations/captions_val2014.json\"\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if GET_EMBEDDING:\n    len(ru_capt), len(coco_train), len(coco_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if GET_EMBEDDING:\n\n    import io\n    import os\n    import PIL\n    import random\n    import numpy as np\n    import torch\n    import torchvision\n    import transformers\n    import more_itertools\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from tqdm import tqdm\n    import pandas as pd\n    from torch.utils.data import Dataset\n    from tqdm import tqdm\n    from dataclasses import dataclass, field\n    import torchvision.transforms as T\n    import torchvision.transforms.functional as TF\n    import cv2\n    from PIL import Image\n    import pickle\n    from tqdm.contrib import tzip\n\n    def read_image(path, size=(196, 196)):\n        image = cv2.imread(path)\n\n        # size = 196, 196\n        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        image.thumbnail(size, Image.Resampling.LANCZOS)\n\n        return image","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:02:57.956876Z","iopub.execute_input":"2023-12-11T08:02:57.957191Z","iopub.status.idle":"2023-12-11T08:02:58.193383Z","shell.execute_reply.started":"2023-12-11T08:02:57.957166Z","shell.execute_reply":"2023-12-11T08:02:58.192433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if GET_EMBEDDING:\n    read_image('/kaggle/input/train2014/train2014/COCO_train2014_000000000030.jpg')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:02:58.194834Z","iopub.execute_input":"2023-12-11T08:02:58.195575Z","iopub.status.idle":"2023-12-11T08:02:58.273294Z","shell.execute_reply.started":"2023-12-11T08:02:58.195538Z","shell.execute_reply":"2023-12-11T08:02:58.272473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if GET_EMBEDDING:\n    first = len(coco_train)//3\n    second = 2*len(coco_train)//3\n    third = len(coco_train)\n\n    steps = [(0, first), (first, second), (second, third)]","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:02:58.274617Z","iopub.execute_input":"2023-12-11T08:02:58.274972Z","iopub.status.idle":"2023-12-11T08:02:58.280325Z","shell.execute_reply.started":"2023-12-11T08:02:58.274940Z","shell.execute_reply":"2023-12-11T08:02:58.279326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if GET_EMBEDDING:\n    all_embeddings = []\n    all_captions = []\n    all_image_id = []\n\n    for start, stop in steps:\n        for i in tqdm(range(start, stop)):\n\n            #image = coco_train[i][0]\n\n            name_img = str(coco_train[i][1][0]['image_id'])\n            name_img = \"0\" * (12 - len(name_img)) + name_img\n            path_img = f\"/kaggle/input/train2014/train2014/COCO_train2014_{name_img}.jpg\"\n\n            image = read_image(path = path_img)\n            image = preprocess(image).unsqueeze(0).to(device)\n            with torch.no_grad():\n                prefix = clip_model.encode_image(image).detach().to(device)\n\n            for capt_sample in coco_train[i][1]:\n                caption = ru_capt[capt_sample['id']]\n\n                text = f\"Вопрос: что на изображении? Ответ: {caption['caption']}\"\n                # TODO: более аккуратный вопрос\n\n                all_captions.append(text)\n                all_embeddings.append(prefix)\n                all_image_id.append(name_img)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:02:58.281307Z","iopub.execute_input":"2023-12-11T08:02:58.281558Z","iopub.status.idle":"2023-12-11T08:02:58.291843Z","shell.execute_reply.started":"2023-12-11T08:02:58.281535Z","shell.execute_reply":"2023-12-11T08:02:58.290867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_path = f\"Features_train_coco_ru_vitb16_82783.pkl\"\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:02:58.293225Z","iopub.execute_input":"2023-12-11T08:02:58.293509Z","iopub.status.idle":"2023-12-11T08:02:58.306635Z","shell.execute_reply.started":"2023-12-11T08:02:58.293480Z","shell.execute_reply":"2023-12-11T08:02:58.305843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if GET_EMBEDDING:\n\n    with open(out_path, 'wb') as f:\n        pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions, 'image_id': all_image_id}, f)\n\n    print('Done')\n    print(\"%0d embeddings saved \" % len(all_embeddings))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:02:58.307760Z","iopub.execute_input":"2023-12-11T08:02:58.308091Z","iopub.status.idle":"2023-12-11T08:02:58.316530Z","shell.execute_reply.started":"2023-12-11T08:02:58.308058Z","shell.execute_reply":"2023-12-11T08:02:58.315738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if GET_EMBEDDING:\n\n    all_embeddings = []\n    all_captions = []\n    all_image_id = []\n\n\n    for i in tqdm(range(0, len(coco_val))):\n        name_img = str(coco_val[i][1][0]['image_id'])\n        name_img = \"0\" * (12 - len(name_img)) + name_img\n        path_img = f\"/kaggle/input/val2014/val2014/COCO_val2014_{name_img}.jpg\"\n\n        image = read_image(path = path_img)\n\n        image = preprocess(image).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            prefix = clip_model.encode_image(image).detach().to(device)\n\n        for capt_sample in coco_val[i][1]:\n            caption = ru_capt[capt_sample['id']]\n\n            text = f\"Вопрос: что на изображении? Ответ: {caption['caption']}\"\n            # TODO: более аккуратный вопрос\n            all_captions.append(text)\n            all_embeddings.append(prefix)\n            all_image_id.append(name_img)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:02:58.317575Z","iopub.execute_input":"2023-12-11T08:02:58.317870Z","iopub.status.idle":"2023-12-11T08:02:58.326853Z","shell.execute_reply.started":"2023-12-11T08:02:58.317839Z","shell.execute_reply":"2023-12-11T08:02:58.326095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(ru_capt)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:02:58.327996Z","iopub.execute_input":"2023-12-11T08:02:58.328263Z","iopub.status.idle":"2023-12-11T08:02:58.340768Z","shell.execute_reply.started":"2023-12-11T08:02:58.328240Z","shell.execute_reply":"2023-12-11T08:02:58.339951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_out_path = \"Features_val_coco_ru_vitb16.pkl\"\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:02:58.341779Z","iopub.execute_input":"2023-12-11T08:02:58.342043Z","iopub.status.idle":"2023-12-11T08:02:58.349739Z","shell.execute_reply.started":"2023-12-11T08:02:58.342020Z","shell.execute_reply":"2023-12-11T08:02:58.348989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if GET_EMBEDDING:\n    with open(val_out_path, 'wb') as f:\n        pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions, 'image_id': all_image_id}, f)\n\n    print('Done')\n    print(\"%0d embeddings saved \" % len(all_embeddings))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:02:58.350802Z","iopub.execute_input":"2023-12-11T08:02:58.351076Z","iopub.status.idle":"2023-12-11T08:02:58.359536Z","shell.execute_reply.started":"2023-12-11T08:02:58.351053Z","shell.execute_reply":"2023-12-11T08:02:58.358737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Открыть данные","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:35:12.313037Z","iopub.execute_input":"2023-12-11T09:35:12.313321Z","iopub.status.idle":"2023-12-11T09:35:13.288311Z","shell.execute_reply.started":"2023-12-11T09:35:12.313297Z","shell.execute_reply":"2023-12-11T09:35:13.287267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#out_path = f\"Features_train_coco_ru_vitb16_82783.pkl\"\nout_path = \"/kaggle/input/coco-ru-image-and-capt/Features_train_coco_ru_vitb16_82783.pkl\"","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:07.566184Z","iopub.execute_input":"2023-12-11T12:07:07.566457Z","iopub.status.idle":"2023-12-11T12:07:07.570968Z","shell.execute_reply.started":"2023-12-11T12:07:07.566431Z","shell.execute_reply":"2023-12-11T12:07:07.569842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#val_out_path = \"Features_val_coco_ru_vitb16.pkl\"\nval_out_path = \"/kaggle/input/coco-ru-image-and-capt/Features_val_coco_ru_vitb16.pkl\"","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:07.572025Z","iopub.execute_input":"2023-12-11T12:07:07.572289Z","iopub.status.idle":"2023-12-11T12:07:07.600471Z","shell.execute_reply.started":"2023-12-11T12:07:07.572264Z","shell.execute_reply":"2023-12-11T12:07:07.599698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\nimport io\nclass CPU_Unpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        if module == 'torch.storage' and name == '_load_from_bytes':\n            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n        else: return super().find_class(module, name)\n\n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:07.601458Z","iopub.execute_input":"2023-12-11T12:07:07.601711Z","iopub.status.idle":"2023-12-11T12:07:07.611569Z","shell.execute_reply.started":"2023-12-11T12:07:07.601689Z","shell.execute_reply":"2023-12-11T12:07:07.610836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(out_path, 'rb') as f:\n    #embeddings_train = pickle.load(f)\n    \n    embeddings_train = CPU_Unpickler(f).load()\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:07.612416Z","iopub.execute_input":"2023-12-11T12:07:07.612682Z","iopub.status.idle":"2023-12-11T12:07:17.451523Z","shell.execute_reply.started":"2023-12-11T12:07:07.612659Z","shell.execute_reply":"2023-12-11T12:07:17.450698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(val_out_path, 'rb') as f:\n    #embeddings_val = pickle.load(f)\n    \n    embeddings_val = CPU_Unpickler(f).load()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:17.452645Z","iopub.execute_input":"2023-12-11T12:07:17.452933Z","iopub.status.idle":"2023-12-11T12:07:20.910611Z","shell.execute_reply.started":"2023-12-11T12:07:17.452909Z","shell.execute_reply":"2023-12-11T12:07:20.909612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(embeddings_train['clip_embedding']), len(embeddings_train['captions'])","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:20.911792Z","iopub.execute_input":"2023-12-11T12:07:20.912123Z","iopub.status.idle":"2023-12-11T12:07:20.918758Z","shell.execute_reply.started":"2023-12-11T12:07:20.912096Z","shell.execute_reply":"2023-12-11T12:07:20.917805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(embeddings_val['clip_embedding']), len(embeddings_val['captions'])","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:36:56.357904Z","iopub.execute_input":"2023-12-11T09:36:56.358218Z","iopub.status.idle":"2023-12-11T09:36:56.366879Z","shell.execute_reply.started":"2023-12-11T09:36:56.358193Z","shell.execute_reply":"2023-12-11T09:36:56.365795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#path_emb_train_coco = f\"Features_train_coco_ru_vitb16_82783.pkl\"\n#path_emb_val_coco = f\"Features_val_coco_ru_vitb16.pkl\"\n\n\npath_emb_train_coco = f\"/kaggle/input/coco-ru-image-and-capt/Features_train_coco_ru_vitb16_82783.pkl\"\npath_emb_val_coco = f\"/kaggle/input/coco-ru-image-and-capt/Features_val_coco_ru_vitb16.pkl\"","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:20.919924Z","iopub.execute_input":"2023-12-11T12:07:20.920226Z","iopub.status.idle":"2023-12-11T12:07:20.929489Z","shell.execute_reply.started":"2023-12-11T12:07:20.920201Z","shell.execute_reply":"2023-12-11T12:07:20.928528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nfrom PIL import Image\n\nclass ClipCocoDataset(Dataset):\n    def __init__(\n        self,\n        data_path: str,\n        prefix_length=30,\n        model_type = gpt_model_name,\n        normalize_prefix=False,\n        train=True,\n    ):\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n        self.prefix_length = prefix_length\n        self.normalize_prefix = normalize_prefix\n        self.train = train\n        if train:\n            with open(data_path, 'rb') as f:\n                all_data = CPU_Unpickler(f).load()#pickle.load(f)\n            print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n        else:\n            with open(data_path, 'rb') as f:\n                all_data = CPU_Unpickler(f).load()#pickle.load(f)\n            print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n\n        sys.stdout.flush()\n        self.prefixes = all_data[\"clip_embedding\"]\n        captions_raw = all_data[\"captions\"]\n        \n        self.captions = captions_raw\n\n        self.image_id = all_data[\"image_id\"]\n        \n        self.captions_tokens = []\n        self.caption2embedding = []\n        max_seq_len = 0\n        i = 0\n        for caption in tqdm(captions_raw):\n            self.captions_tokens.append(\n                torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64)\n            )\n            self.caption2embedding.append(self.prefixes[i])\n            i += 1\n            max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n\n        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n\n    def pad_tokens(self, item: int):\n        tokens = self.captions_tokens[item]\n        padding = self.max_seq_len - tokens.shape[0]\n        if padding > 0:\n            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n           # self.captions_tokens[item] = tokens\n        elif padding < 0:\n            tokens = tokens[:self.max_seq_len]\n           # self.captions_tokens[item] = tokens\n        mask = tokens.ge(0)  # mask is zero where we out of sequence\n        tokens[~mask] = 0\n        mask = mask.float()\n        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n        return tokens, mask\n    \n    def get_image(self, item):\n        if self.train:\n            path_img = f\"/kaggle/input/train2014/train2014/COCO_train2014_{self.image_id[item]}.jpg\"\n        else:\n            path_img = f\"/kaggle/input/val2014/val2014/COCO_val2014_{self.image_id[item]}.jpg\"\n\n        # path_img  =  '/kaggle/input/train2014/train2014/COCO_train2014_000000000030.jpg'\n    \n        image = cv2.imread(path_img)\n        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        image.thumbnail((196, 196), Image.Resampling.LANCZOS)\n        return image\n    \n    def __len__(self) -> int:\n        #if self.train:\n        return len(self.captions_tokens)\n        #else:\n         #   return 5000\n\n    def __getitem__(self, item):\n        tokens, mask = self.pad_tokens(item)\n        prefix = self.prefixes[item]\n        if self.normalize_prefix:\n            prefix = prefix.float()\n            prefix = prefix / prefix.norm(2, -1)\n        return tokens, mask, prefix","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:20.930634Z","iopub.execute_input":"2023-12-11T12:07:20.930934Z","iopub.status.idle":"2023-12-11T12:07:21.173414Z","shell.execute_reply.started":"2023-12-11T12:07:20.930898Z","shell.execute_reply":"2023-12-11T12:07:21.172452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.cuda.get_device_name(0)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:03:00.824766Z","iopub.execute_input":"2023-12-11T08:03:00.825204Z","iopub.status.idle":"2023-12-11T08:03:00.836943Z","shell.execute_reply.started":"2023-12-11T08:03:00.825150Z","shell.execute_reply":"2023-12-11T08:03:00.836204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.cuda.mem_get_info()[1] / 1024**3","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:03:00.838165Z","iopub.execute_input":"2023-12-11T08:03:00.838536Z","iopub.status.idle":"2023-12-11T08:03:00.847518Z","shell.execute_reply.started":"2023-12-11T08:03:00.838500Z","shell.execute_reply":"2023-12-11T08:03:00.846545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import bitsandbytes as bnb","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:03:00.848803Z","iopub.execute_input":"2023-12-11T08:03:00.849169Z","iopub.status.idle":"2023-12-11T08:03:00.857593Z","shell.execute_reply.started":"2023-12-11T08:03:00.849102Z","shell.execute_reply":"2023-12-11T08:03:00.856781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Запусти ячейку и появится окошко для ввода токена)**","metadata":{}},{"cell_type":"code","source":"class wandb:\n    def log(*args, **kwargs):\n        pass","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:03:00.858730Z","iopub.execute_input":"2023-12-11T08:03:00.859001Z","iopub.status.idle":"2023-12-11T08:03:00.867540Z","shell.execute_reply.started":"2023-12-11T08:03:00.858977Z","shell.execute_reply":"2023-12-11T08:03:00.866589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:07:21.174572Z","iopub.execute_input":"2023-12-11T12:07:21.174877Z","iopub.status.idle":"2023-12-11T12:08:17.049665Z","shell.execute_reply.started":"2023-12-11T12:07:21.174835Z","shell.execute_reply":"2023-12-11T12:08:17.048525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\n\nwandb.init(project=\"ClipCap_NAS\", name= 'Selfsupervised')","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:08:22.454691Z","iopub.execute_input":"2023-12-11T12:08:22.455574Z","iopub.status.idle":"2023-12-11T12:08:53.917691Z","shell.execute_reply.started":"2023-12-11T12:08:22.455537Z","shell.execute_reply":"2023-12-11T12:08:53.916735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate\n!pip install torchmetrics","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:09:34.910170Z","iopub.execute_input":"2023-12-11T12:09:34.910918Z","iopub.status.idle":"2023-12-11T12:09:59.267696Z","shell.execute_reply.started":"2023-12-11T12:09:34.910876Z","shell.execute_reply":"2023-12-11T12:09:59.266524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers.optimization import AdamW,Adafactor, AdafactorSchedule\n\nimport os\nimport pickle\nimport sys\nimport argparse\nimport evaluate\n\nfrom typing import Tuple, Optional, Union\nfrom torch.cuda.amp import autocast\nimport torch.nn.functional as F\n\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:09:59.270370Z","iopub.execute_input":"2023-12-11T12:09:59.270781Z","iopub.status.idle":"2023-12-11T12:10:01.987746Z","shell.execute_reply.started":"2023-12-11T12:09:59.270737Z","shell.execute_reply":"2023-12-11T12:10:01.986671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torchmetrics.functional.multimodal import clip_score\n# from functools import partial\n# from torchvision import transforms\n# clip_score_fn = partial(clip_score, model_name_or_path=clip_model)\n\n# def calculate_clip_score(images, prompts):\n#     transform = transforms.Compose([transforms.PILToTensor()])\n#     images_int = transform(images)\n#     clip_score = clip_score_fn(images_int, prompts).detach()\n#     return round(float(clip_score), 4)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T06:05:00.149749Z","iopub.execute_input":"2023-12-11T06:05:00.151328Z","iopub.status.idle":"2023-12-11T06:05:00.543556Z","shell.execute_reply.started":"2023-12-11T06:05:00.151290Z","shell.execute_reply":"2023-12-11T06:05:00.542484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:41:16.695801Z","iopub.execute_input":"2023-12-11T09:41:16.696902Z","iopub.status.idle":"2023-12-11T09:41:28.850153Z","shell.execute_reply.started":"2023-12-11T09:41:16.696858Z","shell.execute_reply":"2023-12-11T09:41:28.848946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from accelerate import Accelerator, notebook_launcher\n# from accelerate.utils import set_seed ","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:41:28.852941Z","iopub.execute_input":"2023-12-11T09:41:28.853647Z","iopub.status.idle":"2023-12-11T09:41:28.864882Z","shell.execute_reply.started":"2023-12-11T09:41:28.853603Z","shell.execute_reply":"2023-12-11T09:41:28.863776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set_seed(manualSeed)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:41:28.866092Z","iopub.execute_input":"2023-12-11T09:41:28.867561Z","iopub.status.idle":"2023-12-11T09:41:28.881137Z","shell.execute_reply.started":"2023-12-11T09:41:28.867509Z","shell.execute_reply":"2023-12-11T09:41:28.880262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def supervised_train(\n    train_dataset: ClipCocoDataset,\n    valid_dataset: ClipCocoDataset,\n    model: ClipCaptionModel,\n    train_dataloader,\n    valid_dataloader,\n    optimizer,\n    scheduler,\n    args,\n #   warmup_steps: int = 2000,\n    output_dir: str = \".\",\n    output_prefix: str = \"\",\n    metric: bool = False,\n):\n    #\n    batch_size = args.bs\n    epochs = args.epochs\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    model = freeze(model)\n    \n    metric_dataset = valid_dataset\n\n#     train_dataloader = DataLoader(\n#         train_dataset,\n#         batch_size=batch_size,\n#         shuffle=True,\n#         drop_last=True,\n#     )\n    \n\n#     valid_dataloader = DataLoader(\n#         valid_dataset,\n#         batch_size=batch_size,\n#         shuffle=False,\n#         drop_last=False,\n#     )\n    \n#     scheduler = get_linear_schedule_with_warmup(\n#         optimizer,\n#         num_warmup_steps=warmup_steps,\n#         num_training_steps=epochs * len(train_dataloader)\n#     )\n    \n#     optimizer = AdamW(\n#         model.parameters(),\n#         lr=args.lr,\n#        # relative_step=False, # for adafactor\n#     )\n    \n    #scheduler = AdafactorSchedule(optimizer) работает не оч\n\n    mean_epoch_train_loss = []\n    mean_epoch_validation_loss = []\n    \n    bleu_metric = []\n    clipscore_metric = []\n    \n    bleu = evaluate.load(\"bleu\")\n    \n    metric_len = 1000\n    \n    #accelerator = Accelerator(mixed_precision=\"fp16\")\n    #accelerator = Accelerator()\n    #model, optimizer, train_dataloader,valid_dataloader, scheduler = accelerator.prepare(model, optimizer, train_dataloader,valid_dataloader, scheduler)\n\n    model.train()\n\n    for epoch in range(epochs):\n        loss_train_epoch = []\n     #   loss_valid_epoch = []\n        print(f\">>> Training epoch {epoch+1}\")\n        sys.stdout.flush()\n        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n        step=0\n        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n            model.zero_grad()\n            step += 1\n            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.bfloat16)\n            \n            outputs = model(tokens, prefix, mask)\n            logits = outputs.logits[:, train_dataset.prefix_length - 1: -1]\n\n#             probabilities = F.gumbel_softmax(logits, dim=-1)\n            \n#             loss = nnf.cross_entropy(\n#                 probabilities.reshape(-1, logits.shape[-1]),\n#                 tokens.flatten().to(torch.int64),\n#                 ignore_index=0\n#             )  \n            \n            loss = nnf.cross_entropy(\n                logits.reshape(-1, logits.shape[-1]),\n                tokens.flatten().to(torch.int64),\n                ignore_index=0\n            )     \n                \n            loss.backward()\n          #  accelerator.backward(loss)\n            optimizer.step()\n            scheduler.step()\n\n            optimizer.zero_grad()\n            progress.set_postfix({\"loss_train\": loss.item()})\n            \n            loss_train_epoch.append(loss.item())\n\n            if step % 500:\n                wandb.log({\"loss_train\":  loss.item()})\n            \n            progress.update()\n            if (idx + 1) % 7000 == 0:\n                torch.save(\n                    model.state_dict(),\n                    os.path.join(output_dir, f\"{output_prefix}_latest_gpt2_medium.pt\"),\n                )\n          #      accelerator.save_model(model, os.path.join(output_dir, f\"{output_prefix}_latest_gpt2_medium.pt\"))\n                            \n        progress.close()\n        if epoch % args.save_every == 0:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}-{(epoch+1):03d}_gpt2_medium.pt\"),\n            )\n\n        if metric:\n            print(f\">>> Metric {epoch+1}\")\n\n           # progress = tqdm(total=len(valid_dataloader), desc=output_prefix)\n            progress = tqdm(total=metric_len, desc=output_prefix)\n            step_validation=0\n            bleu_result = 0\n            clipscore_result = 0\n            for idx, (tokens, mask, prefix) in enumerate(valid_dataloader):\n                step_validation+=1\n\n                if batch_size * step_validation >= metric_len:\n                    break\n\n                with torch.no_grad():\n\n                   # tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.bfloat16)\n\n                    outputs = model(\n                            question_tokens, prefix, q_mask\n                        )\n\n                    logits = outputs.logits\n\n                    for b in range(batch_size):\n\n                        generated_text = valid_dataset.tokenizer.decode(logits[b].argmax(dim=-1).tolist())\n\n                        bias = generated_text.find('Ответ: ')\n                        if bias != -1:\n                            generated_text = generated_text[bias+7:]\n                        references = [valid_dataset.captions[idx * 2 + b][35:]]\n\n                        bleu_result += bleu.compute(predictions=[generated_text], references=references)['bleu']\n                       # clipscore_result += calculate_clip_score(metric_dataset.get_image(idx), generated_text)      \n\n\n                progress.set_postfix({\"bluescore_result\": bleu_result/step_validation})\n\n                #loss_valid_epoch.append(loss.item())\n                if step_validation % 500:\n                    wandb.log({\"bleu\":  bleu_result/step_validation})\n                 #   wandb.log({\"clipscore\": clipscore_result/step_validation})\n\n                progress.update()\n                \n            bleu_metric.append(bleu_result / metric_len)\n            wandb.log({\"epoch_blue\": bleu_metric[-1]})\n           # clipscore_metric.append(clipscore_result / metric_len)\n           #    wandb.log({\"epoch_clipscope\": clipscore_metric[-1]})\n\n        mean_epoch_train_loss.append(np.mean(loss_train_epoch))\n        wandb.log({\"mean_epoch_train_loss\": mean_epoch_train_loss[-1]})\n\n\n        progress.close()        \n        \n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:10:01.989192Z","iopub.execute_input":"2023-12-11T12:10:01.989987Z","iopub.status.idle":"2023-12-11T12:10:02.016631Z","shell.execute_reply.started":"2023-12-11T12:10:01.989949Z","shell.execute_reply":"2023-12-11T12:10:02.015712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Args():\n    def __init__(self):\n        self.backbone = gpt_model_name\n        self.train_data = \"/kaggle/input/coco-ru-image-and-capt/Features_train_coco_ru_vitb16_82783.pkl\"\n        self.valid_data = \"/kaggle/input/coco-ru-image-and-capt/Features_val_coco_ru_vitb16.pkl\"\n        self.out_dir = 'checkpoints'\n        self.prefix = 'first_start'\n        self.epochs = 3\n        self.save_every = 1\n        self.prefix_length = 30\n        self.bs = 2\n        self.only_prefix = False\n        self.lr = 2e-5\n        self.warmup_steps = 2000\n        \nargs = Args()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:10:05.842418Z","iopub.execute_input":"2023-12-11T12:10:05.843060Z","iopub.status.idle":"2023-12-11T12:10:05.851351Z","shell.execute_reply.started":"2023-12-11T12:10:05.843027Z","shell.execute_reply":"2023-12-11T12:10:05.850173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = ClipCocoDataset(args.train_data, args.prefix_length, train=True)\nvalid_dataset = ClipCocoDataset(args.valid_data, args.prefix_length, train=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:10:09.693900Z","iopub.execute_input":"2023-12-11T12:10:09.694816Z","iopub.status.idle":"2023-12-11T12:13:37.452931Z","shell.execute_reply.started":"2023-12-11T12:10:09.694778Z","shell.execute_reply":"2023-12-11T12:13:37.451926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_tokens = torch.tensor(tokenizer.encode(\"Вопрос: что на изображении? Ответ:\"), dtype=torch.int64)\nquestion_tokens = torch.cat((question_tokens, torch.zeros(valid_dataset.max_seq_len - question_tokens.shape[0], dtype=torch.int64) - 1))\nq_mask = question_tokens.ge(0)  # mask is zero where we out of sequence\nquestion_tokens[~q_mask] = 0\nq_mask = q_mask.float()\nq_mask = torch.cat((torch.ones(valid_dataset.prefix_length), q_mask), dim=0)  # adding prefix mask\nquestion_tokens = torch.stack([question_tokens] * args.bs).to(device)\nq_mask = torch.stack([q_mask] * args.bs).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:07:04.851094Z","iopub.execute_input":"2023-12-11T16:07:04.851544Z","iopub.status.idle":"2023-12-11T16:07:04.866922Z","shell.execute_reply.started":"2023-12-11T16:07:04.851506Z","shell.execute_reply":"2023-12-11T16:07:04.865894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split\nsup_train_dataset, unsup_train_dataset = random_split(train_dataset, [0.05, 0.95], generator=torch.Generator().manual_seed(manualSeed))","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:14:03.606929Z","iopub.execute_input":"2023-12-11T12:14:03.607280Z","iopub.status.idle":"2023-12-11T12:14:03.657982Z","shell.execute_reply.started":"2023-12-11T12:14:03.607250Z","shell.execute_reply":"2023-12-11T12:14:03.656825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(sup_train_dataset), len(unsup_train_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:14:07.437215Z","iopub.execute_input":"2023-12-11T12:14:07.437555Z","iopub.status.idle":"2023-12-11T12:14:07.445631Z","shell.execute_reply.started":"2023-12-11T12:14:07.437528Z","shell.execute_reply":"2023-12-11T12:14:07.444395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sup_train_dataset.prefix_length = train_dataset.prefix_length\nunsup_train_dataset.prefix_length = train_dataset.prefix_length","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:14:12.562031Z","iopub.execute_input":"2023-12-11T12:14:12.562886Z","iopub.status.idle":"2023-12-11T12:14:12.568277Z","shell.execute_reply.started":"2023-12-11T12:14:12.562841Z","shell.execute_reply":"2023-12-11T12:14:12.567127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.config = {\n  \"learning_rate\": args.lr,\n  \"epochs\": args.epochs,\n  \"batch_size\": args.bs\n}\n\n# model = ClipCaptionModel(args.prefix_length)\n# model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:14:15.173284Z","iopub.execute_input":"2023-12-11T12:14:15.174152Z","iopub.status.idle":"2023-12-11T12:14:15.179522Z","shell.execute_reply.started":"2023-12-11T12:14:15.174116Z","shell.execute_reply":"2023-12-11T12:14:15.178625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1dm13xsso06sj382Hz8Qx-BBMySE6myG9' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1dm13xsso06sj382Hz8Qx-BBMySE6myG9\" -O first_start-001_gpt2_medium.pt && rm -rf /tmp/cookies.txt","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:14:43.207524Z","iopub.execute_input":"2023-12-11T12:14:43.208035Z","iopub.status.idle":"2023-12-11T12:15:24.357750Z","shell.execute_reply.started":"2023-12-11T12:14:43.207994Z","shell.execute_reply":"2023-12-11T12:15:24.356564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:46:28.378847Z","iopub.execute_input":"2023-12-11T09:46:28.379722Z","iopub.status.idle":"2023-12-11T09:46:29.465982Z","shell.execute_reply.started":"2023-12-11T09:46:28.379684Z","shell.execute_reply":"2023-12-11T09:46:29.464853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelpath = 'first_start-001_gpt2_medium.pt'\n#modelpath = '/kaggle/input/1119-11-dec/first_start-001_gpt2_medium.pt'\n\nmodel = ClipCaptionModel(args.prefix_length)\nmodel.load_state_dict(torch.load(modelpath, map_location='cpu')) \nmodel = model.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:15:36.879216Z","iopub.execute_input":"2023-12-11T12:15:36.880182Z","iopub.status.idle":"2023-12-11T12:16:02.138961Z","shell.execute_reply.started":"2023-12-11T12:15:36.880146Z","shell.execute_reply":"2023-12-11T12:16:02.137946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sup_train_dataloader = DataLoader(\n    sup_train_dataset,\n    batch_size=args.bs,\n    shuffle=True,\n    drop_last=True,\n)\n\n\nvalid_dataloader = DataLoader(\n    valid_dataset,\n    batch_size=args.bs,\n    shuffle=False,\n    drop_last=False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:16:07.214253Z","iopub.execute_input":"2023-12-11T12:16:07.214935Z","iopub.status.idle":"2023-12-11T12:16:07.222464Z","shell.execute_reply.started":"2023-12-11T12:16:07.214893Z","shell.execute_reply":"2023-12-11T12:16:07.221335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(\n    model.parameters(),\n    lr=args.lr,\n   # relative_step=False, # for adafactor\n)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=args.warmup_steps,\n    num_training_steps=args.epochs * len(sup_train_dataloader)\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:16:10.170018Z","iopub.execute_input":"2023-12-11T12:16:10.170723Z","iopub.status.idle":"2023-12-11T12:16:10.184335Z","shell.execute_reply.started":"2023-12-11T12:16:10.170692Z","shell.execute_reply":"2023-12-11T12:16:10.183151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bleu = evaluate.load(\"bleu\")\n\nfor idx, (tokens, mask, prefix) in enumerate(sup_train_dataloader):\n    with torch.no_grad():\n        tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.bfloat16)\n        outputs = model(\n                tokens,prefix, mask\n            )\n\n        logits = outputs.logits\n        \n        for b in range(args.bs):\n\n            generated_text = train_dataset.tokenizer.decode(logits[b][29:].argmax(dim=-1).tolist())\n#             bias = generated_text.find('Ответ: ')\n#             if bias != -1:\n#                 generated_text = generated_text[bias+7:]\n            #references = [train_dataset.captions[idx * args.bs + b][35:]]\n            references = [train_dataset.tokenizer.decode(tokens[0].tolist())]\n            print(generated_text)\n            print(references)\n            print(bleu.compute(predictions=[generated_text], references=references))\n            break\n        break","metadata":{"execution":{"iopub.status.busy":"2023-12-11T18:24:42.310189Z","iopub.execute_input":"2023-12-11T18:24:42.311033Z","iopub.status.idle":"2023-12-11T18:24:43.344022Z","shell.execute_reply.started":"2023-12-11T18:24:42.310994Z","shell.execute_reply":"2023-12-11T18:24:43.342788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# valid_dataset.get_image(0)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:10:19.497214Z","iopub.status.idle":"2023-12-10T20:10:19.497843Z","shell.execute_reply.started":"2023-12-10T20:10:19.497510Z","shell.execute_reply":"2023-12-10T20:10:19.497541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/checkpoints","metadata":{"execution":{"iopub.status.busy":"2023-12-11T18:03:21.825191Z","iopub.execute_input":"2023-12-11T18:03:21.826052Z","iopub.status.idle":"2023-12-11T18:03:22.909557Z","shell.execute_reply.started":"2023-12-11T18:03:21.826018Z","shell.execute_reply":"2023-12-11T18:03:22.908290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm /kaggle/working/first_start-003_gpt2_medium.pt","metadata":{"execution":{"iopub.status.busy":"2023-12-11T18:03:25.087012Z","iopub.execute_input":"2023-12-11T18:03:25.087460Z","iopub.status.idle":"2023-12-11T18:03:26.653687Z","shell.execute_reply.started":"2023-12-11T18:03:25.087418Z","shell.execute_reply":"2023-12-11T18:03:26.651474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv /kaggle/working/checkpoints/first_start-003_gpt2_medium.pt /kaggle/working/\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T18:03:29.468898Z","iopub.execute_input":"2023-12-11T18:03:29.469307Z","iopub.status.idle":"2023-12-11T18:03:30.558987Z","shell.execute_reply.started":"2023-12-11T18:03:29.469271Z","shell.execute_reply":"2023-12-11T18:03:30.557622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r /kaggle/working/checkpoints/*","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:37:48.950812Z","iopub.execute_input":"2023-12-11T10:37:48.951868Z","iopub.status.idle":"2023-12-11T10:37:50.081521Z","shell.execute_reply.started":"2023-12-11T10:37:48.951823Z","shell.execute_reply":"2023-12-11T10:37:50.080351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:16:59.649826Z","iopub.execute_input":"2023-12-11T16:16:59.650228Z","iopub.status.idle":"2023-12-11T16:17:00.550903Z","shell.execute_reply.started":"2023-12-11T16:16:59.650195Z","shell.execute_reply":"2023-12-11T16:17:00.549823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train both prefix and GPT2\")\nsys.stdout.flush()\nmodel = supervised_train(\n    sup_train_dataset,\n    valid_dataset,\n    model,\n    sup_train_dataloader,\n    valid_dataloader,\n    optimizer,\n    scheduler,\n    args,\n    output_dir=args.out_dir,\n    output_prefix=args.prefix,\n    metric = False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:17:13.209599Z","iopub.execute_input":"2023-12-11T16:17:13.210029Z","iopub.status.idle":"2023-12-11T18:02:39.894977Z","shell.execute_reply.started":"2023-12-11T16:17:13.209991Z","shell.execute_reply":"2023-12-11T18:02:39.893362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Train both prefix and GPT2\")\n# sys.stdout.flush()\n# notebook_launcher(supervised_train, \n#     (sup_train_dataset,\n#     valid_dataset,\n#     model,\n#     sup_train_dataloader,\n#     valid_dataloader,\n#     optimizer,\n#     scheduler,\n#     args,\n#     args.out_dir,\n#     args.prefix,\n#     False,\n# ), num_processes=2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unsupervised_train(\n    train_dataset: ClipCocoDataset,\n    valid_dataset: ClipCocoDataset,\n    model: ClipCaptionModel,\n    args,\n    warmup_steps: int = 2000,\n    output_dir: str = \".\",\n    output_prefix: str = \"\",\n    metric: bool = False,\n):\n    #\n    batch_size = args.bs\n    epochs = args.epochs\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    model = freeze(model)\n    \n    optimizer = Adafactor(\n        model.parameters(),\n        lr=args.lr,\n        relative_step=False, # for adafactor\n    )\n\n    metric_dataset = valid_dataset\n\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n    )\n    \n    valid_dataloader = DataLoader(\n        valid_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        drop_last=False,\n    )\n    \n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=epochs * len(train_dataloader)\n    )\n    \n    #scheduler = AdafactorSchedule(optimizer) работает не оч\n\n    mean_epoch_train_loss = []\n    mean_epoch_validation_loss = []\n    \n    bleu_metric = []\n    clipscore_metric = []\n    \n    bleu = evaluate.load(\"bleu\")\n    metric_len = 1000\n\n        \n    model.train()\n\n    for epoch in range(epochs):\n        loss_train_epoch = []\n     #   loss_valid_epoch = []\n        print(f\">>> Training epoch {epoch+1}\")\n        sys.stdout.flush()\n        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n        step=0\n        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n            model.zero_grad()\n            step += 1\n            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.bfloat16)\n            \n            outputs = model(tokens, prefix, mask)\n\n            last_hiddens_state = outputs.hidden_states[-1]\n            image_emb = model.clip_project(prefix.float()).view(-1, model.prefix_length, model.gpt_embedding_size)\n            embedding_text = model.gpt.transformer.wte(tokens)\n            image_emb = torch.cat((image_emb, embedding_text), dim=1)\n            loss = - torch.sum(F.cosine_similarity(image_emb,\n                                                   last_hiddens_state, dim=-1))\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            optimizer.zero_grad()\n            progress.set_postfix({\"loss_train\": loss.item()})\n            \n            loss_train_epoch.append(loss.item())\n\n            if step % 500:\n                wandb.log({\"unsupervised_loss_train\":  loss.item()})\n            \n            progress.update()\n            if (idx + 1) % 7000 == 0:\n                torch.save(\n                    model.state_dict(),\n                    os.path.join(output_dir, f\"{output_prefix}_latest_gpt2_medium.pt\"),\n                )\n                \n                \n        progress.close()\n        if epoch % args.save_every == 0:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}-{(epoch+1):03d}_gpt2_medium.pt\"),\n            )\n\n        if metric:\n            print(f\">>> Metric {epoch+1}\")\n\n            progress = tqdm(total=metric_len, desc=output_prefix)\n            step_validation=0\n            bleu_result = 0\n            clipscore_result = 0\n            for idx, (tokens, mask, prefix) in enumerate(valid_dataloader):\n                step_validation+=1\n                if step_validation * batch_size >= metric_len:\n                    break\n                with torch.no_grad():\n\n                    tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.bfloat16)\n\n\n                    outputs = model(\n                            question_tokens, prefix, q_mask\n                        )\n\n                    logits = outputs.logits\n\n                    for b in range(batch_size):\n\n                        generated_text = valid_dataset.tokenizer.decode(logits[b].argmax(dim=-1).tolist())\n\n                        bias = generated_text.find('Ответ: ')\n                        if bias != -1:\n                            generated_text = generated_text[bias+7:]\n                        references = [valid_dataset.captions[idx * 2 + b][35:]]\n\n                        bleu_result += bleu.compute(predictions=[generated_text], references=references)['bleu']\n                       # clipscore_result += calculate_clip_score(metric_dataset.get_image(idx), generated_text)      \n\n\n                progress.set_postfix({\"blue_result\": bleu_result/step_validation})\n\n                #loss_valid_epoch.append(loss.item())\n                if step_validation % 500:\n                    wandb.log({\"unsupervised_bleu\":  bleu_result/step_validation})\n                  #  wandb.log({\"clipscore\": clipscore_result/step_validation})\n\n                progress.update()\n                \n            bleu_metric.append(bleu_result / metric_len)\n          #   clipscore_metric.append(clipscore_result / metric_len)\n          #  wandb.log({\"epoch_clipscope\": clipscore_metric[-1]})\n            wandb.log({\"unsupervised_epoch_blue\": bleu_metric[-1]})\n\n        mean_epoch_train_loss.append(np.mean(loss_train_epoch))\n        wandb.log({\"unsupervised_mean_epoch_train_loss\": mean_epoch_train_loss[-1]})\n        progress.close()\n        \n        \n    return model\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train both prefix and GPT2\")\nsys.stdout.flush()\nmodel = unsupervised_train(\n    unsup_train_dataset,\n    valid_dataset,\n    model,\n    args,\n    output_dir=args.out_dir,\n    output_prefix=args.prefix,\n    metric = False,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}