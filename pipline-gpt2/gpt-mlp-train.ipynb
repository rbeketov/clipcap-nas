{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7194192,"sourceType":"datasetVersion","datasetId":4160414},{"sourceId":7209000,"sourceType":"datasetVersion","datasetId":4170992}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wandb\n# !pip install bitsandbytes\n# !pip install ruclip==0.0.2\n!pip install transformers==4.27.4\n# !pip install pycocotools\n# !pip install git+https://github.com/openai/CLIP.git\n# !pip install open_clip_torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport os\nimport pickle\nimport sys\nimport argparse\nimport json\n#import ruclip\n#import clip, open_clip\nimport random\nimport io\n#import bitsandbytes as bnb\nimport wandb\nimport nltk\nimport numpy as np\n\nfrom nltk.translate.bleu_score import corpus_bleu\n\nfrom sklearn.model_selection import train_test_split\n\nfrom datasets import load_dataset, load_metric\n\nfrom torch.nn import functional as nnf\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast\nfrom torch.utils.data import Subset\n\n\nfrom transformers import GPT2Config, GPT2Model\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers.optimization import Adafactor, AdafactorSchedule\n\nfrom typing import Tuple, Optional, Union\nfrom tqdm import tqdm, trange\nfrom enum import Enum","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#manualSeed = 1337\n#manualSeed = random.randint(1, 10000) # use if you want new results\n#random.seed(manualSeed)\n#torch.manual_seed(manualSeed)\n#\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MappingType(Enum):\n    MLP = 'mlp'\n    Transformer = 'transformer'\n\nclass MLP(nn.Module):\n    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n        super(MLP, self).__init__()\n        layers = []\n        for i in range(len(sizes) - 1):\n            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n            if i < len(sizes) - 2:\n                layers.append(act())\n        self.model = nn.Sequential(*layers)\n    \n    @autocast()  \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef freeze(\n    model,\n    freeze_emb=False,\n    freeze_ln=True,\n    freeze_attn=True,\n    freeze_ff=True,\n    freeze_other=False,\n):\n    \n    for name, p in model.named_parameters():\n    # freeze all parameters except the layernorm and positional embeddings\n        name = name.lower()\n        if 'ln' in name or 'norm' in name:\n            p.requires_grad = not freeze_ln\n        elif 'embeddings' in name:\n            p.requires_grad = not freeze_emb\n        elif 'mlp' in name:\n            p.requires_grad = not freeze_ff\n        elif 'attn' in name:\n            p.requires_grad = not freeze_attn\n        else:\n            p.requires_grad = not freeze_other\n           \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpt_model_name = 'sberbank-ai/rugpt3medium_based_on_gpt2'\nclass ClipCaptionModel(nn.Module):\n    def __init__(\n        self,\n        prefix_length: int,\n        clip_length: Optional[int] = None,\n        prefix_size: int = 512,\n        num_layers: int = 8,\n        mapping_type: MappingType = MappingType.MLP\n    ):\n        super(ClipCaptionModel, self).__init__()\n        self.prefix_length = prefix_length\n\n        self.gpt = GPT2LMHeadModel.from_pretrained(gpt_model_name)\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n\n        if mapping_type == MappingType.MLP:\n            self.clip_project = MLP((\n                prefix_size,\n                self.gpt_embedding_size * prefix_length // 2,\n                self.gpt_embedding_size * prefix_length\n            ))\n        else:\n            self.clip_project = TransformerMapper(\n                prefix_size,\n                self.gpt_embedding_size,\n                prefix_length,\n                clip_length, \n                num_layers\n            )\n\n        \n    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n    \n    @autocast() \n    def forward(\n        self,        \n        tokens: torch.Tensor,\n        prefix: torch.Tensor,\n        mask: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None\n    ):\n        embedding_text = self.gpt.transformer.wte(tokens)\n        prefix_projections = self.clip_project(\n            prefix.float()\n        ).view(-1, self.prefix_length, self.gpt_embedding_size)\n\n        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n\n        if labels is not None:\n            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n            labels = torch.cat((dummy_token, tokens), dim=1)\n        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask, output_hidden_states = True)\n        \n        return out\n\nclass ClipCaptionPrefix(ClipCaptionModel):\n    def parameters(self, recurse: bool = True):\n        return self.clip_project.parameters()\n\n    def train(self, mode: bool = True):\n        super(ClipCaptionPrefix, self).train(mode)\n        self.gpt.eval()\n        return self","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CPU_Unpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        if module == 'torch.storage' and name == '_load_from_bytes':\n            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n        else: return super().find_class(module, name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClipCocoDataset(Dataset):\n    def __init__(\n        self,\n        data_path: str,\n        prefix_length=30,\n        model_type = gpt_model_name,\n        normalize_prefix=False,\n        train=True,\n    ):\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n        self.prefix_length = prefix_length\n        self.normalize_prefix = normalize_prefix\n        if train:\n            with open(data_path, 'rb') as f:\n                all_data = CPU_Unpickler(f).load() #pickle.load(f)\n            print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n        else:\n            with open(data_path, 'rb') as f:\n                all_data = CPU_Unpickler(f).load() #pickle.load(f)\n            print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n\n        sys.stdout.flush()\n        self.prefixes = all_data[\"clip_embedding\"]\n        captions_raw = all_data[\"captions\"]\n        \n        self.captions = captions_raw\n\n        self.image_id = all_data[\"path_images\"]\n\n        self.captions_tokens = []\n        self.caption2embedding = []\n        max_seq_len = 0\n        i = 0\n        for caption in tqdm(captions_raw):\n            self.captions_tokens.append(\n                torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64)\n            )\n            self.caption2embedding.append(self.prefixes[i])\n            i += 1\n            max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n\n        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n\n    def get_image(self, item):\n        if self.train:\n            path_img = f\"/kaggle/input/train2014/train2014/{self.image_id[item]}\"\n        else:\n            path_img = f\"/kaggle/input/val2014/val2014/{self.image_id[item]}\"\n            \n        image = cv2.imread(path_img)\n        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        image.thumbnail((196, 196), Image.Resampling.LANCZOS)\n        return image\n    \n    def pad_tokens(self, item: int):\n        tokens = self.captions_tokens[item]\n        padding = self.max_seq_len - tokens.shape[0]\n        if padding > 0:\n            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            #self.captions_tokens[item] = tokens\n        elif padding < 0:\n            tokens = tokens[:self.max_seq_len]\n            #self.captions_tokens[item] = tokens\n        mask = tokens.ge(0)  # mask is zero where we out of sequence\n        tokens[~mask] = 0\n        mask = mask.float()\n        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n        return tokens, mask\n    \n    def __len__(self) -> int:\n        return len(self.captions_tokens)\n\n    def __getitem__(self, item):\n        tokens, mask = self.pad_tokens(item)\n        prefix = self.prefixes[item]\n        if self.normalize_prefix:\n            prefix = prefix.float()\n            prefix = prefix / prefix.norm(2, -1)\n        return tokens, mask, prefix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_bleu(y_pred, y_true):\n    references = [[reference.split()] for reference in y_true]\n    hypotheses = [hypothesis.split() for hypothesis in y_pred]\n    # Рассчитываем BLEU-4\n    bleu_score = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n    return bleu_score*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **TRAIN LOOP**","metadata":{}},{"cell_type":"code","source":"def train(\n    train_dataset: ClipCocoDataset,\n    train_dataloader,\n    model: ClipCaptionModel,\n    optimizer,\n    scheduler,\n    args,\n    warmup_steps: int = 5000,\n    output_dir: str = \".\",\n    output_prefix: str = \"\",   \n):\n\n    batch_size = args.bs\n    epochs = args.epochs\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    model.train()\n    \n\n   \n    mean_epoch_train_loss = []\n    mean_bleu_train_epoch = []\n    \n    \n    for epoch in range(epochs):\n        loss_train_epoch = []\n        bleu_train_epoch = []\n        print(f\">>> Training epoch {epoch+1}\")\n        sys.stdout.flush()\n        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n        step=0\n        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n            model.zero_grad()\n            step += 1\n            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.bfloat16)\n            \n            outputs = model(tokens, prefix, mask)\n            logits = outputs.logits[:, train_dataset.prefix_length - 1: -1]\n\n            loss = nnf.cross_entropy(\n                logits.reshape(-1, logits.shape[-1]),\n                tokens.flatten().to(torch.int64),\n                ignore_index=0\n            )\n\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            loss_train_epoch.append(loss.item())\n            optimizer.zero_grad()\n\n            progress.set_postfix({\"supervised_loss_train\": loss.item()})\n    \n            if step % 500 == 0:\n                wandb.log({\"supervised_loss_train\": loss.item()})\n                wandb.log({\"mean_supervised_loss_train\":  np.mean(loss_train_epoch)})\n            \n            if step % 1000 == 0:\n                with torch.no_grad():\n                    # BLEU-4\n                    logits_cpu = logits.cpu()\n                    tokens_cpu = tokens.cpu()\n                    generated_texts = []\n                    real_text = []\n                    for b in range(batch_size):\n                        generated_text_batch = train_dataset.tokenizer.decode(logits_cpu[b].argmax(dim=-1).tolist())\n                        first_dot_index = generated_text_batch.find('.')\n                        if first_dot_index != -1:\n                            generated_texts.append(generated_text_batch[35:first_dot_index + 1])\n                        else:\n                            generated_texts.append(generated_text_batch[35:])\n                        \n                        real_text_batch = train_dataset.tokenizer.decode(tokens_cpu[b].tolist())\n                        first_pad_index = real_text_batch.find('<pad>')\n                        if first_pad_index != -1:\n                            real_text.append(real_text_batch[35:first_pad_index])\n                        else:\n                            real_text.append(real_text_batch[35:])\n                    \n                    bleu = calc_bleu(generated_texts, real_text)\n                    wandb.log({\"supervised_bleu-4_train\":  bleu})\n                    bleu_train_epoch.append(bleu)\n                    wandb.log({\"mean_supervised_bleu-4_train\": np.mean(bleu_train_epoch)})\n\n            progress.update()\n            if (idx + 1) % 7000 == 0:\n                torch.save(\n                    model.state_dict(),\n                    os.path.join(output_dir, f\"{output_prefix}_latest_gpt2_medium.pt\"),\n                )\n        progress.close()\n        if epoch % args.save_every == 0:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}-{(epoch+1):03d}_gpt2_medium.pt\"),\n            )\n        mean_epoch_train_loss.append(np.mean(loss_train_epoch))\n        mean_bleu_train_epoch.append(np.mean(bleu_train_epoch))\n        \n        wandb.log({\"mean_epoch_sup_train_loss\": mean_epoch_train_loss[-1]})\n        wandb.log({\"mean_bleu_sup_train_epoch\": mean_bleu_train_epoch[-1]})\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(project=\"ClipCap_NAS\", name=\"ruclip-gpt-mlp-train\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Args():\n    def __init__(self):\n        self.backbone = gpt_model_name\n        self.train_data = \"/kaggle/input/coco2014-ru-clip-embeddings/embeddings_ru_clip_train.pkl\"\n        self.valid_data = \"/kaggle/input/coco2014-ru-clip-embeddings/embeddings_ru_clip_valid.pkl\"\n        self.out_dir = 'checkpoints'\n        self.prefix = 'gpt'\n        self.epochs = 1\n        self.save_every = 1\n        self.prefix_length = 30\n        self.bs = 1\n        self.only_prefix = False\n        self.lr = 2e-5\n        self.warmup_steps = 5000\nargs = Args()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = ClipCocoDataset(args.train_data, args.prefix_length, train=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.config = {\n  \"learning_rate\": args.lr,\n  \"epochs\": args.epochs,\n  \"batch_size\": args.bs\n}\n\n\n\nmodel = ClipCaptionModel(args.prefix_length)\nmodel_path = '/kaggle/input/prefix-mlp-weights-clipcap/checkpoints/only_prefix_latest_gpt2_medium.pt'\nmodel.load_state_dict(torch.load(model_path, map_location='cpu'))\nmodel = freeze(model)\nmodel = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    train_dataset,\n    batch_size=args.bs,\n    shuffle=True,\n    drop_last=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(\n        model.parameters(),\n        lr=args.lr,\n    )\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=args.warmup_steps,\n    num_training_steps=args.epochs * len(train_dataloader)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train(\n    train_dataset,\n    train_dataloader,\n    model,\n    optimizer,\n    scheduler,\n    args,\n    warmup_steps=args.warmup_steps,\n    output_dir=args.out_dir,\n    output_prefix=args.prefix\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip weights_gpt.zip checkpoints/gpt_latest_gpt2_medium.pt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c =10000000\nwhile c > 0:\n    c-= 1","metadata":{"execution":{"iopub.status.busy":"2023-12-16T02:25:08.321646Z","iopub.execute_input":"2023-12-16T02:25:08.322128Z","iopub.status.idle":"2023-12-16T02:25:09.574100Z","shell.execute_reply.started":"2023-12-16T02:25:08.322095Z","shell.execute_reply":"2023-12-16T02:25:09.573036Z"},"trusted":true},"execution_count":38,"outputs":[]}]}